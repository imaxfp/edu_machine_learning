{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- A new idea emerged to copy the biological brain for artificial intelligence.\n",
    "- Nature-inspired algorithms led to the development of neural networks and deep learning.\n",
    "- AI is now on the cusp of a new golden age with machines that rival human intelligence.\n",
    "- Traditional approaches to computing cannot mimic the intelligence of birds and bees.\n",
    "- A bee has around 950,000 neurons.\n",
    "- Neural networks emerged as a biologically inspired solution.\n",
    "- Neural networks are now the foundation of powerful AI technology, like Google's Deepmind.\n",
    "- This guide is about understanding and creating neural networks for difficult tasks like recognizing human handwriting.\n",
    "\n",
    "\n",
    "\n",
    "<a id='Forward propagation explained'></a>\n",
    "\n",
    "\n",
    "\n",
    "###Forward propagation explained\n",
    "\n",
    "#### What will we do?\n",
    "In this book we’ll take a journey to making a neural network that can recognise human handwritten numbers.\n",
    "\n",
    "We’ll journey through mathematical ideas like functions, simple linear classifiers, iterative refinement, matrix multiplication, gradient calculus, optimisation through gradient descent and even geometric rotations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A Simple Predicting Machine\n",
    "\n",
    "<img src=\"./img/kilometrs_miles.png\" width=\"700\"/>\n",
    "\n",
    "imagine we don’t know the formula for converting between kilometres and miles. All we know is the relationship between is linear.\n",
    "Mysterious calculation it needs to be of the form 'miles = kilometres * C', where 'C' is a constant. We don’t know what this constant 'C' is yet.\n",
    "\n",
    "What should we do to work out that missing constant c? Let’s just pluck a value at random and give it a go! Let’s try c = 0.5 and see what happens.\n",
    "\n",
    "<img src=\"./img/miles_kilometrs_err_1.png\" width=\"700\"/>\n",
    "\n",
    "difference between our calculated answer and the actual truth 12,137\n",
    "\n",
    "<img src=\"./img/miles_kilometrs_err_2.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/miles_kilometrs_err_3.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Classifying is Not Very Different from Predicting\n",
    "We called the above simple machine a predictor, because it takes an input and makes a prediction of what the output should be.\n",
    "\n",
    "\n",
    "<img src=\"./img/lr_divider_1.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "The adjustable parameter 'C' changed the slope of that straight line. We can use the line to separate different kinds of things.\n",
    "\n",
    "<img src=\"./img/lr_divider_2.png\" width=\"500\"/>\n",
    "\n",
    "For now, we’re simply trying to illustrate the idea of a simple classifier.\n",
    "How do we get the right slope? How do we improve a line we know isn’t a good divider between the two kinds of bugs?\n",
    "\n",
    "T - target (labeled data)\n",
    "Y - predicted\n",
    "E - error\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training A Simple Classifier\n",
    "\n",
    "<img src=\"./img/training_data.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization.png\" width=\"500\"/>\n",
    "\n",
    "Looking back at our miles to kilometre predictor, we had a linear function whose parameter we adjusted.\n",
    "Dividing line is a straight line \"y = Ax\"\n",
    "You may also notice that this \"y = A * x\" is simpler than the fuller form for a straight line \"y = Ax + B\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s go for **'A' is 0.25** to get started. The dividing line is **'y = 0.25x'** Let’s plot this line on the same plot of training data to see what it looks like:\n",
    "\n",
    "<img src=\"./img/training_data_visualization_2.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization_3.png\" width=\"500\"/>\n",
    "\n",
    "How is **A** related to **E**? If we can know this, then we can understand how **changing one affects the other.**\n",
    "Mathematicians use the delta symbol Δ to mean “a small change in”. Let’s write that out:\n",
    "\n",
    "t = (A + ΔA)x\n",
    "Let’s picture this to make it easier to understand. You can see the new slope **(A+ ΔA)**\n",
    "\n",
    "<img src=\"./img/training_data_visualization_4.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization_explained.png\" width=\"700\"/>\n",
    "\n",
    "T - target (labeled data)\n",
    "Y - predicted value (dependent value)\n",
    "E - error\n",
    "x - feature (independent value)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sometimes One Classifier Is Not Enough\n",
    "Illustrate the limit of a linear classifier with a simple but stark example.\n",
    "Boolean logic functions - AND and OR.\n",
    "Boolean logical functions typically take two inputs and output one answer:\n",
    "\n",
    "<img src=\"./img/logical_ftion.png\" width=\"500\"/>\n",
    "\n",
    "There is another Boolean function called XOR, short for eXclusive OR, which only has a true output if either one of the inputs A or B is true, but not both.\n",
    "\n",
    "<img src=\"./img/XOR.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/logical_XOR_visual.png\" width=\"500\"/>\n",
    "\n",
    "A and B to the logical function as coordinates on a graph. The plot shows that only when both are true, with value (1,1), is the output also true, shown as green. False outputs are shown red."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Representing Boolean Functions with Linear Classification\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neurons, Nature’s Computing Machines\n",
    "The very capable human brain has about 100 billion neurons!\n",
    "A biological neuron doesn’t produce an output that is simply a simple linear function of the input.\n",
    "\n",
    "Observations suggest that neurons don’t react readily, but instead suppress the input until it has grown so large that it triggers an output. You can think of this as a threshold that must be reached before any output is produced. It’s like water in a cup - the water doesn’t spill over until it has first filled the cup. Intuitively this makes sense - the neurons don’t want to be passing on tiny noise signals, only emphatically strong intentional signals. The following illustrates this idea of only producing an output signal if the input is sufficiently dialed up to pass a threshold.\n",
    "\n",
    "***Output=(constant∗input)+(maybe another constant)***\n",
    "\n",
    "<img src=\"./img/neuron_threshold.png\" width=\"500\"/>\n",
    "\n",
    "A function that takes the input signal and generates an output signal, but takes into account some kind of threshold is called an **activation function**.\n",
    "\n",
    "A simple step function could do this:\n",
    "\n",
    "<img src=\"./img/step_fun.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/sigmoid_fun.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"./img/sigmoid_formula.png\" width=\"500\"/>\n",
    "\n",
    "e - a mathematical constant 2.71828…\n",
    "x - The input x is negated and e is raised to the power of that -x. (The result is added to 1, so we have 1+e^-x)\n",
    "\n",
    "when x is zero, e^−x is one because anything raised to a power of zero is 1. So y becomes 1/(1+1) or simply 1/2, a half. So the basic sigmoid cuts the y-axis at y=1/2.\n",
    "\n",
    "<img src=\"./img/sigmoid_formula_detailed.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing to realize is that real biological neurons take many inputs, not just one. We saw this when we had two inputs to the Boolean logic machine, so the idea of having more than one input is not new or unusual.\n",
    "\n",
    "We simply combine them by adding them up, and the resultant sum is the input to the sigmoid function which controls the output. This reflects how real neurons work. The following diagram illustrates this idea of combining inputs and then applying the threshold to the combined sum:\n",
    "\n",
    "<img src=\"./img/sum_inputs.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/neron_bio_comp.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/neron_comput.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "If only one of the several inputs is large and the rest small, this may be enough to fire the neuron.\n",
    "\n",
    "What’s more, the neuron can fire if some of the inputs are individually almost, but not quite, large enough because when combined the signal is large enough to overcome the threshold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The electrical signals are collected by the dendrites and these combine to form a stronger electrical signal. If the signal is strong enough to pass the threshold, the neuron fires a signal**\n",
    "\n",
    "<img src=\"./img/bio_nn.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most obvious thing is to adjust the strength of the connections between nodes. Within a node, we could have adjusted the summation of the inputs\n",
    "\n",
    "The following diagram again shows the connected nodes, but this time a weight is shown associated with each connection.\n",
    "\n",
    "A low weight will de-emphasise a signal, and a high weight will amplify(посилить) it.\n",
    "\n",
    "<img src=\"./img/nn_layers_weighted.png\" width=\"600\"/>\n",
    "\n",
    "What do we mean by this? It means that as the network learns to improve its outputs by refining the link weights inside the network\n",
    "\n",
    "Some weights become zero or close to zero. Zero, or almost zero, weights means those links don’t contribute to the network because signals don’t pass."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO\n",
    "\n",
    "<img src=\"./img/nn_example1.png\" width=\"600\"/>\n",
    "\n",
    "Random starting values aren’t such a bad idea, and it is what we did when we chose an initial slope value for the simple linear classifiers earlier on.\n",
    "\n",
    "- The first layer of nodes is the input layer, and it doesn’t do anything other than represent the input signals. That is, the input nodes don’t apply an activation function to the input.\n",
    "- The first layer of neural networks is the input layer and all that layer does is represent the inputs that’s it.\n",
    "- the second layer where we do need to do some calculations. For each node in this layer we need to work out the combined input. Remember that sigmoid function **y = 1/(1+e^-x)**\n",
    "\n",
    "<img src=\"./img/second_layer_explained.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/second_layer_formula_explained.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/second_layer_formula_explained2.png\" width=\"600\"/>\n",
    "\n",
    "The process will be repeated for the all layers\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Matrix Multiplication is Useful\n",
    "A matrix is just a table, a rectangular grid, of numbers. That’s it. There’s nothing much more complex about a matrix than that.\n",
    "\n",
    "Here’s an example of two simple matrices multiplied together.\n",
    "\n",
    "<img src=\"./img/matrics_multiplication.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/matrics_multiplication2.png\" width=\"600\"/>\n",
    "\n",
    "So you can’t multiply a “2 by 2” matrix by a “5 by 5” matrix.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Look what happens if we replace the letters with words that are more meaningful to our neural networks. The second matrix is a two by one matrix, but the multiplication approach is the same.\n",
    "\n",
    "<img src=\"./img/matrixs_mult_nn.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/nn_matrix_inputs.png\" width=\"700\"/>\n",
    "\n",
    "We can express all the calculations that go into working out the combined moderated signal, x, into each node of the second layer using matrix multiplication\n",
    "\n",
    "<center>\n",
    "X=W∗I\n",
    "</center>\n",
    "\n",
    "W - the matrix of weights\n",
    "I - the matrix of inputs\n",
    "X - the resultant matrix of combined moderated signals into layer 2\n",
    "\n",
    "This is fantastic! A little bit of effort to understand matrix multiplication has given us a powerful tool for implementing neural networks without lots of effort from us."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward propagation explained\n",
    "TODO img\n",
    "\n",
    "Input layer:\n",
    "- X1=1\n",
    "- X2=2\n",
    "\n",
    "Hidden layer:\n",
    "    * Node 1: w1,0=0.1, w1,1=0.2, b=0\n",
    "    * Node 2: w1,0=0.4, w1,1=0.5, b=0\n",
    "    * Node 3: w1,0=1.1, w1,1=1.2, b=0\n",
    "\n",
    "Output layer:\n",
    "    * w2,0=1, w2,1=1.2, w2,2=1.4\n",
    "\n",
    "1. Calculate the weighted sum and the activation function output for each node in the hidden layer:\n",
    "    * Node 1:\n",
    "    * Weighted sum = (0.1 * 1) + (0.2 * 2) + 0 = 0.5\n",
    "    * Activation function output = sigmoid(0.5) = 0.6225\n",
    "    * Node 2:\n",
    "    * Weighted sum = (0.4 * 1) + (0.5 * 2) + 0 = 1.4\n",
    "    * Activation function output = sigmoid(1.4) = 0.8022\n",
    "    * Node 3:\n",
    "    * Weighted sum = (1.1 * 1) + (1.2 * 2) + 0 = 3.5\n",
    "    * Activation function output = sigmoid(3.5) = 0.9707\n",
    "\n",
    "2. Calculate the weighted sum and the activation function output for the output node:\n",
    "* Weighted sum = (1 * 0.6225) + (1.2 * 0.8022) + (1.4 * 0.9707) = 3.0206\n",
    "* Activation function output = sigmoid(3.0206) = 0.9531\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "\n",
    "TODO:\n",
    "\n",
    "**activation function?** That’s easy and doesn’t need matrix multiplication. All we need to do is apply the sigmoid function to each individual element of the matrix X.\n",
    "\n",
    "**soft max**\n",
    "\n",
    "**Relu**\n",
    "<br>\n",
    "<img src=\"./img/activation_functions.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backpropagation explained\n",
    "Backpropagation is an algorithm used to train neural networks by adjusting the weights. It works by propagating the error in the output of the network backwards through the layers to adjust the weights in each layer.\n",
    "\n",
    "TODO:\n",
    "http://neuralnetworksanddeeplearning.com/chap2.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning Weights From More Than One Node\n",
    "How do we update link weights when more than one node contributes to an output and its error? The following illustrates this problem.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/otput_layer.png\" width=\"700\"/>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Backpropagating Errors From More Output Nodes\n",
    "\n",
    "<img src=\"./img/err_calculation.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/w_err_calc.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_explained.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Backpropagating Errors To More Layers\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_err_2.png\" width=\"800\"/>\n",
    "\n",
    "<br>\n",
    "Working back from the final output layer at the right hand side\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_err_2_details.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_err_3.png\" width=\"800\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "We don’t have the target or desired outputs for the hidden nodes. We only have the target values for the final output layer nodes, and these come from the training examples. That means we have some kind of error for each of the two links that emerge from this middle layer node. We could recombine these two link errors to form the error for this node as a second best approach because we don’t actually have a target value for the middle layer node.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_err_4.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"./img/backpropagating_err_5.png\" width=\"800\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}