{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[Go to Cell 2](#cell-2)\n",
    "[Go to Cell 2](#cell-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "- A new idea emerged to copy the biological brain for artificial intelligence.\n",
    "- Nature-inspired algorithms led to the development of neural networks and deep learning.\n",
    "- AI is now on the cusp of a new golden age with machines that rival human intelligence.\n",
    "- Traditional approaches to computing cannot mimic the intelligence of birds and bees.\n",
    "- A bee has around 950,000 neurons.\n",
    "- Neural networks emerged as a biologically inspired solution.\n",
    "- Neural networks are now the foundation of powerful AI technology, like Google's Deepmind.\n",
    "- This guide is about understanding and creating neural networks for difficult tasks like recognizing human handwriting.\n",
    "\n",
    "#### What will we do?\n",
    "In this book we’ll take a journey to making a neural network that can recognise human handwritten numbers.\n",
    "\n",
    "We’ll journey through mathematical ideas like functions, simple linear classifiers, iterative refinement, matrix multiplication, gradient calculus, optimisation through gradient descent and even geometric rotations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Simple Predicting Machine\n",
    "\n",
    "<img src=\"./img/kilometrs_miles.png\" width=\"700\"/>\n",
    "\n",
    "imagine we don’t know the formula for converting between kilometres and miles. All we know is the relationship between is linear.\n",
    "Mysterious calculation it needs to be of the form 'miles = kilometres * C', where 'C' is a constant. We don’t know what this constant 'C' is yet.\n",
    "\n",
    "What should we do to work out that missing constant c? Let’s just pluck a value at random and give it a go! Let’s try c = 0.5 and see what happens.\n",
    "\n",
    "<img src=\"./img/miles_kilometrs_err_1.png\" width=\"700\"/>\n",
    "\n",
    "difference between our calculated answer and the actual truth 12,137\n",
    "\n",
    "<img src=\"./img/miles_kilometrs_err_2.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/miles_kilometrs_err_3.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "{## Classifying is Not Very Different from Predicting}\n",
    "We called the above simple machine a predictor, because it takes an input and makes a prediction of what the output should be.\n",
    "\n",
    "\n",
    "<img src=\"./img/lr_divider_1.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "The adjustable parameter 'C' changed the slope of that straight line. We can use the line to separate different kinds of things.\n",
    "\n",
    "<img src=\"./img/lr_divider_2.png\" width=\"500\"/>\n",
    "\n",
    "For now, we’re simply trying to illustrate the idea of a simple classifier.\n",
    "How do we get the right slope? How do we improve a line we know isn’t a good divider between the two kinds of bugs?\n",
    "\n",
    "T - target (labeled data)\n",
    "Y - predicted\n",
    "E - error\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training A Simple Classifier\n",
    "\n",
    "<img src=\"./img/training_data.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization.png\" width=\"500\"/>\n",
    "\n",
    "Looking back at our miles to kilometre predictor, we had a linear function whose parameter we adjusted.\n",
    "Dividing line is a straight line \"y = Ax\"\n",
    "You may also notice that this \"y = A * x\" is simpler than the fuller form for a straight line \"y = Ax + B\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s go for **'A' is 0.25** to get started. The dividing line is **'y = 0.25x'** Let’s plot this line on the same plot of training data to see what it looks like:\n",
    "\n",
    "<img src=\"./img/training_data_visualization_2.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization_3.png\" width=\"500\"/>\n",
    "\n",
    "How is **A** related to **E**? If we can know this, then we can understand how **changing one affects the other.**\n",
    "Mathematicians use the delta symbol Δ to mean “a small change in”. Let’s write that out:\n",
    "\n",
    "t = (A + ΔA)x\n",
    "Let’s picture this to make it easier to understand. You can see the new slope **(A+ ΔA)**\n",
    "\n",
    "<img src=\"./img/training_data_visualization_4.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/training_data_visualization_explained.png\" width=\"700\"/>\n",
    "\n",
    "T - target (labeled data)\n",
    "Y - predicted value (dependent value)\n",
    "E - error\n",
    "x - feature (independent value)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sometimes One Classifier Is Not Enough\n",
    "Illustrate the limit of a linear classifier with a simple but stark example.\n",
    "Boolean logic functions - AND and OR.\n",
    "Boolean logical functions typically take two inputs and output one answer:\n",
    "\n",
    "<img src=\"./img/logical_ftion.png\" width=\"500\"/>\n",
    "\n",
    "There is another Boolean function called XOR, short for eXclusive OR, which only has a true output if either one of the inputs A or B is true, but not both.\n",
    "\n",
    "<img src=\"./img/XOR.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/logical_XOR_visual.png\" width=\"500\"/>\n",
    "\n",
    "A and B to the logical function as coordinates on a graph. The plot shows that only when both are true, with value (1,1), is the output also true, shown as green. False outputs are shown red."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Representing Boolean Functions with Linear Classification\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Neurons, Nature’s Computing Machines\n",
    "The very capable human brain has about 100 billion neurons!\n",
    "A biological neuron doesn’t produce an output that is simply a simple linear function of the input.\n",
    "\n",
    "Observations suggest that neurons don’t react readily, but instead suppress the input until it has grown so large that it triggers an output. You can think of this as a threshold that must be reached before any output is produced. It’s like water in a cup - the water doesn’t spill over until it has first filled the cup. Intuitively this makes sense - the neurons don’t want to be passing on tiny noise signals, only emphatically strong intentional signals. The following illustrates this idea of only producing an output signal if the input is sufficiently dialed up to pass a threshold.\n",
    "\n",
    "***Output=(constant∗input)+(maybe another constant)***\n",
    "\n",
    "<img src=\"./img/neuron_threshold.png\" width=\"500\"/>\n",
    "\n",
    "A function that takes the input signal and generates an output signal, but takes into account some kind of threshold is called an **activation function**.\n",
    "\n",
    "A simple step function could do this:\n",
    "\n",
    "<img src=\"./img/step_fun.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"./img/sigmoid_fun.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"./img/sigmoid_formula.png\" width=\"500\"/>\n",
    "\n",
    "e - a mathematical constant 2.71828…\n",
    "x - The input x is negated and e is raised to the power of that -x. (The result is added to 1, so we have 1+e^-x)\n",
    "\n",
    "when x is zero, e^−x is one because anything raised to a power of zero is 1. So y becomes 1/(1+1) or simply 1/2, a half. So the basic sigmoid cuts the y-axis at y=1/2.\n",
    "\n",
    "<img src=\"./img/sigmoid_formula_detailed.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing to realize is that real biological neurons take many inputs, not just one. We saw this when we had two inputs to the Boolean logic machine, so the idea of having more than one input is not new or unusual.\n",
    "\n",
    "We simply combine them by adding them up, and the resultant sum is the input to the sigmoid function which controls the output. This reflects how real neurons work. The following diagram illustrates this idea of combining inputs and then applying the threshold to the combined sum:\n",
    "\n",
    "<img src=\"./img/sum_inputs.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/neron_bio_comp.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/neron_comput.png\" width=\"600\"/>\n",
    "\n",
    "\n",
    "If only one of the several inputs is large and the rest small, this may be enough to fire the neuron.\n",
    "\n",
    "What’s more, the neuron can fire if some of the inputs are individually almost, but not quite, large enough because when combined the signal is large enough to overcome the threshold."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The electrical signals are collected by the dendrites and these combine to form a stronger electrical signal. If the signal is strong enough to pass the threshold, the neuron fires a signal**\n",
    "\n",
    "<img src=\"./img/bio_nn.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most obvious thing is to adjust the strength of the connections between nodes. Within a node, we could have adjusted the summation of the inputs\n",
    "\n",
    "The following diagram again shows the connected nodes, but this time a weight is shown associated with each connection.\n",
    "\n",
    "A low weight will de-emphasise a signal, and a high weight will amplify(посилить) it.\n",
    "\n",
    "<img src=\"./img/nn_layers_weighted.png\" width=\"600\"/>\n",
    "\n",
    "What do we mean by this? It means that as the network learns to improve its outputs by refining the link weights inside the network\n",
    "\n",
    "Some weights become zero or close to zero. Zero, or almost zero, weights means those links don’t contribute to the network because signals don’t pass."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO\n",
    "\n",
    "<img src=\"./img/nn_example1.png\" width=\"600\"/>\n",
    "\n",
    "Random starting values aren’t such a bad idea, and it is what we did when we chose an initial slope value for the simple linear classifiers earlier on.\n",
    "\n",
    "- The first layer of nodes is the input layer, and it doesn’t do anything other than represent the input signals. That is, the input nodes don’t apply an activation function to the input.\n",
    "- The first layer of neural networks is the input layer and all that layer does is represent the inputs that’s it.\n",
    "- the second layer where we do need to do some calculations. For each node in this layer we need to work out the combined input. Remember that sigmoid function **y = 1/(1+e^-x)**\n",
    "\n",
    "<img src=\"./img/second_layer_explained.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/second_layer_formula_explained.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/second_layer_formula_explained2.png\" width=\"600\"/>\n",
    "\n",
    "The process will be repeated for the all layers\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Matrix Multiplication is Useful\n",
    "A matrix is just a table, a rectangular grid, of numbers. That’s it. There’s nothing much more complex about a matrix than that.\n",
    "\n",
    "Here’s an example of two simple matrices multiplied together.\n",
    "\n",
    "<img src=\"./img/matrics_multiplication.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"./img/matrics_multiplication2.png\" width=\"600\"/>\n",
    "\n",
    "So you can’t multiply a “2 by 2” matrix by a “5 by 5” matrix.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Look what happens if we replace the letters with words that are more meaningful to our neural networks. The second matrix is a two by one matrix, but the multiplication approach is the same.\n",
    "\n",
    "<img src=\"./img/matrixs_mult_nn.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"./img/nn_matrix_inputs.png\" width=\"700\"/>\n",
    "\n",
    "We can express all the calculations that go into working out the combined moderated signal, x, into each node of the second layer using matrix multiplication\n",
    "\n",
    "<center>\n",
    "X=W∗I\n",
    "</center>\n",
    "\n",
    "W - the matrix of weights\n",
    "I - the matrix of inputs\n",
    "X - the resultant matrix of combined moderated signals into layer 2\n",
    "\n",
    "This is fantastic! A little bit of effort to understand matrix multiplication has given us a powerful tool for implementing neural networks without lots of effort from us."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Forward propagation explained\n",
    "TODO img\n",
    "\n",
    "Input layer:\n",
    "- X1=1\n",
    "- X2=2\n",
    "\n",
    "Hidden layer:\n",
    "    * Node 1: w1,0=0.1, w1,1=0.2, b=0\n",
    "    * Node 2: w1,0=0.4, w1,1=0.5, b=0\n",
    "    * Node 3: w1,0=1.1, w1,1=1.2, b=0\n",
    "\n",
    "Output layer:\n",
    "    * w2,0=1, w2,1=1.2, w2,2=1.4\n",
    "\n",
    "1. Calculate the weighted sum and the activation function output for each node in the hidden layer:\n",
    "    * Node 1:\n",
    "    * Weighted sum = (0.1 * 1) + (0.2 * 2) + 0 = 0.5\n",
    "    * Activation function output = sigmoid(0.5) = 0.6225\n",
    "    * Node 2:\n",
    "    * Weighted sum = (0.4 * 1) + (0.5 * 2) + 0 = 1.4\n",
    "    * Activation function output = sigmoid(1.4) = 0.8022\n",
    "    * Node 3:\n",
    "    * Weighted sum = (1.1 * 1) + (1.2 * 2) + 0 = 3.5\n",
    "    * Activation function output = sigmoid(3.5) = 0.9707\n",
    "\n",
    "2. Calculate the weighted sum and the activation function output for the output node:\n",
    "* Weighted sum = (1 * 0.6225) + (1.2 * 0.8022) + (1.4 * 0.9707) = 3.0206\n",
    "* Activation function output = sigmoid(3.0206) = 0.9531\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "\n",
    "TODO:\n",
    "\n",
    "**activation function?** That’s easy and doesn’t need matrix multiplication. All we need to do is apply the sigmoid function to each individual element of the matrix X.\n",
    "\n",
    "**soft max**\n",
    "\n",
    "**Relu**\n",
    "<br>\n",
    "<img src=\"./img/activation_functions.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backpropagation explained\n",
    "The backpropagation algorithm was originally introduced in the 1970s. 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. That paper describes several neural networks where backpropagation works far faster than earlier approaches to learning\n",
    "\n",
    "Backpropagation gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network.\n",
    "\n",
    "Backpropagation is an algorithm used to train neural networks by adjusting the weights. It works by propagating the error in the output of the network backwards through the layers to adjust the weights in each layer.\n",
    "\n",
    "#### The Hadamard product\n",
    "Hadamard product or Schur product. In particular, suppose s and t are two vectors of the same dimension. Then we use s⊙t to denote the elementwise product of the two vectors.\n",
    "\n",
    "#### The four fundamental equations behind backpropagation\n",
    "Backpropagation is about understanding how changing the weights and biases in a network changes the cost function. Backpropagation will give us a procedure to compute the error.\n",
    "\n",
    "To understand how the error is defined, imagine there is a demon in our neural network:\n",
    "\n",
    "<img src=\"./img/demon_in_nn.png\" width=\"500\"/>\n",
    "The demon sits at the jth neuron in layer l. As the input to the neuron comes in, the demon messes with the neuron's operation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "TODO:\n",
    "http://neuralnetworksanddeeplearning.com/chap2.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning Weights From More Than One Node\n",
    "How do we update link weights when more than one node contributes to an output and its error?\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/how_to_update_w.png\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "If we have two nodes, how do we use that output error? That error was there because ***more than one link*** contributed to it. There is a tiny chance that only **one link** of many was responsible for the error\n",
    "\n",
    "**One idea is to split the error equally amongst all contributing nodes, as shown next.**\n",
    "<br>\n",
    "<img src=\"./img/split_err_equally.png\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "Another idea is to split the error but not to do it equally.\n",
    "<br>\n",
    "<img src=\"./img/split_err_not_equally.png\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "We can see that 3/4 of the output error should be used to update the first larger weight and that 1/4 of the error for the second smaller weight."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagating Errors From More Output Nodes\n",
    "\n",
    "<img src=\"./img/err_from_more_outputs.png\" width=\"500\"/>\n",
    "\n",
    "Remember this is the difference between the desired output provided by the **training data t1** and the actual **output o1**. err = (t1 - o1)\n",
    "The error at the second output node is labeled e2.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagation: Splitting the Error\n",
    "\n",
    "<img src=\"./img/splitting_the_error.png\" width=\"500\"/>\n",
    "\n",
    "If we had even more layers, we’d repeatedly apply this same idea to each layer working backward from the final output layer.\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/preceding_layer.png\" width=\"500\"/>\n",
    "\n",
    "We need an error for the hidden layer nodes so we can use it to update the weights in the preceding layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagation: Recombining the Error\n",
    "The training data examples only tell us what the outputs from the very **final nodes** should be.\n",
    "They don’t tell us what the outputs from nodes in any **other layer** should be - this is the core of the puzzle.\n",
    "\n",
    "\n",
    "So the error in the first hidden node is the sum of the split errors in all the links connecting forward from the same node.\n",
    "See the diagram above.\n",
    "\n",
    "<img src=\"./img/backprop_formula.png\" width=\"700\"/>\n",
    "\n",
    "Let’s follow one error back. You can see the error 0.5 at the second output layer node being split proportionately into 0.1 and 0.4 across the two connected links which have weights 1.0 and 4.0. You can also see that the recombined error at the second hidden layer node is the sum of the connected split errors, which are 0.48 and 0.4, to give 0.88.\n",
    "\n",
    "<img src=\"./img/back_prop_details.png\" width=\"700\"/>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagating Errors with Matrix Multiplication\n",
    "Here we only have two nodes in the output layer, so these are e1 and e2. Next, we want to construct the matrix for the hidden layer errors.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./img/backpropagation_matrices.png\" width=\"700\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "This is great but did we do the right thing cutting out that normalizing factor? It turns out that this simpler feedback of the error signals works just as well as the more sophisticated one we worked out earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How Do We Actually Update Weights?\n",
    "There are just too many combinations of weights and too many functions of functions of functions\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/sigmoid_sum_for_node.png\" width=\"700\"/>\n",
    "\n",
    "Ok -\n",
    "Σ (sigma)\n",
    "w -\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Understanding the Gradient Descent Algorithm\n",
    "\n",
    "Imagine a very complex landscape with peaks and troughs, and hills with treacherous bumps and gaps. You need to get to the bottom.\n",
    "What do you do? You’ll probably use the torch to look at the area close to your feet. You can’t use it to see much further anyway, and certainly not the entire landscape. You can see which bit of earth seems to be going downhill and take small steps in that direction. In this way, you slowly work your way down the hill, step by step, without having a full map and without having worked out a journey beforehand.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/gradient_descent_cave.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "The mathematical version of this approach is called gradient descent, and you can see why. After you’ve taken a step, you look again at the surrounding area to see which direction takes you closer to your objective, and then you step again in that direction. You keep doing this until you’re happy you’ve arrived at the bottom.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Understanding the Gradient Descent Algorithm\n",
    "\n",
    "Now imagine that complex landscape is a mathematical function. What this gradient descent method gives us is an ability to find the minimum without actually having to understand that complex function enough to work it out mathematically. Going downhill to find the minimum means we are minimizing the error. We are improving the network’s output.\n",
    "\n",
    "The following graph shows a simple function y=(x−1)^2 + 1\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/gradient_descent_increase_x.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "For the finding minimum we look around the place we’re standing and see which direction is downwards. Follow the downward direction, so we move along x to the right. That is, ***we increase x a little***. That’s our hill climber’s first step. You can see that we have improved our position and moved closer to the actual minimum.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/gradient_descent_decrease_x_example.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "This time, the slope beneath our feet is positive, so we move to the left. That is, we **decrease x a little**.\n",
    "\n",
    "<br>\n",
    "<img src=\"./img/gradient_descent_smaller_steps.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Also it is proportionate to the size of the gradient then **when we are close, we’ll take smaller steps**. This assumes that as we get closer to a minimum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}