{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "###  Reinforcement Learning\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/reinforcement_learning.png\" width=\"700\"/>\n",
    "\n",
    "The task is given the position of the helicopter to decide how to move the control sticks. In reinforcement learning, we call the position and orientation and speed and so on of the helicopter the state s.\n",
    "\n",
    "Do you tilt a bit to the left or a lot more to the left or increase the helicopter stress a little bit or a lot? It's actually very difficult to get a data set of x and the ideal action y.\n",
    "\n",
    "The supervised learning approach doesn't work well and we instead use reinforcement learning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/reward_fun_idea.png\" width=\"700\"/>\n",
    "\n",
    "**Good dog bad dog methodology**\n",
    "\n",
    "key input to a reinforcement learning is something called the **reward** or the reward function which tells the helicopter when it's doing well and when it's doing poorly.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/reinforcement_apps.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/reinforcement_apps2.png\" width=\"700\"/>\n",
    "\n",
    " In this example, state 1 here on the left has a very interesting surface that scientists would love for the rover to sample. State 6 also has a pretty interesting surface that scientists would quite like the rover to sample, but not as interesting as state 1. We would more likely to carry out the science mission ant state 1 than at state 6, but state 1 is further away.\n",
    "\n",
    "In reinforcement learning, we pay a lot of attention to the rewards because that's how we know if the robot is doing well or poorly. Let's look at some examples of what might happen if the robot was to go left, starting from state 4. Then initially starting from state 4, it will receive a reward of zero, and after going left, it gets to state 3, where it receives again a reward of zero. Then it gets to state 2, receives the reward is 0, and finally just to state 1, where it receives a reward of 100.\n",
    "\n",
    "every time step, the robot is in some state, which I'll call S, and it gets to choose an action, and it also enjoys some rewards, R of S that it gets from that state. As a result of this action, it to some new state S prime. As a concrete example, when the robot was in state 4 and it took the action, go left, maybe didn't enjoy the reward of zero associated with that state 4 and it won't have any new state 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/discount_factor.png\" width=\"700\"/>\n",
    "\n",
    "The **discount factor** is a number a little bit less than 1. Let me pick 0.9 as the discount factor. I'm going to weight the reward in the first step is just zero, the reward in the second step is a discount factor, 0.9 times that reward, and then plus the discount factor^2 times that reward, and then plus the discount factor^3 times that reward. If you calculate this out, this turns out to be 0.729 times 100, which is 72.9.\n",
    "\n",
    " In financial applications, the discount factor also has a very natural interpretation as the interest rate or the time value of money. If you can have a dollar today, that may be worth a little bit more than if you could only get a dollar in the future.\n",
    "\n",
    "To summarize, the return in reinforcement learning is the sum of the rewards that the system gets, weighted by the discount factor, where rewards in the far future are weighted by the discount factor raised to a higher power. Now, this actually has an interesting effect when you have systems with negative rewards."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Review of key concepts\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/review_of_concepts.png\" width=\"700\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/review_of_concepts2.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/q1.png\" width=\"900\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/q2.png\" width=\"900\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/q3.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/state_action.png\" width=\"900\"/>\n",
    "\n",
    "When the discount factor gamma is 0.5, so Q of S. A will be equal to the total return If you start from say that take the action A and then behave optimally after that. Meaning take actions according to this policy. Shown over here, let's figure out what Q of s,a. Is for a few different states. Let's look at say Q of state too.\n",
    "\n",
    " if you can compute Q of s,a. For every state and every action, then that gives us a good way to compute the auto policy pi of S. So that's the state action value function or the Q function. We'll talk later about how to come up with an algorithm to compute them despite the slightly circular aspect of the definition of the Q function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Belman equation\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/belman_eq.png\" width=\"900\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/belman_eq2.png\" width=\"900\"/>\n",
    "\n",
    " high level intuition I hope you take away is that the total return you get in the reinforcement learning problem has two parts. The first part is this reward that you get right away, and then the second part is Gamma times the return you get starting from the next state s prime. As these two components together, R of s plus Gamma times the return from the next state, that is equal to the total return from the current state s. That is the essence of the Bellman equation.\n",
    "\n",
    "You can still apply Bellman's equations to get a reinforcement learning algorithm to work correctly, but I hope that at least the high level intuition of why breaking down the rewards into what you get right away plus what you get in the future."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Continuous state space applications\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/autonomus_helicopter.png\" width=\"700\"/>\n",
    "\n",
    "To summarize, the state of the helicopter includes is position in the say, north-south direction, is positioned in the east-west direction, y is height above ground, and also the row, the pitch, and also that yaw of helicopter.\n",
    "\n",
    "To write this down, the state therefore includes the position x, y, z, and then the row pitch, and yaw denoted with the Greek alphabets Phi, Theta and Omega. But to control the helicopter, we also need to know its speed in the x-direction, in the y-direction, and in the z direction, as well as its rate of turning, also called the angular velocity. How fast is this row changing and how fast is this pitch changing and how fast is its yaw changing? This is actually the state used to control autonomous helicopters. Is this list of 12 numbers that is input to a policy, and **the job of a policy is look at these 12 numbers and decide what's an appropriate action to take in the helicopter.**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " ### Learning the state-value function\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/bellman_eq01.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/bellman_eq_x_y.png\" width=\"700\"/>\n",
    "\n",
    "But how do you get the training set with values for **'x' and 'y'** that you can then train a neural network on? Let's take a look. Here's the Bellman equation, Q of s, a equals R of s plus Gamma, max of a prime, Q of s prime, a prime. The right-hand side is what you want Q of s, a to be equal to, so I'm going to call this value **on the right-hand side 'y'** and the input to the neural network is a **state and an action so I'm going to call that 'x'.** The job of the neural network is to input x, that is input the state action pair, and try to accurately predict what will be the value on the right."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/bellman_eq_train_prepar.png\" width=\"700\"/>\n",
    "\n",
    "We're going to use the lunar lander and just try taking different actions in it. If we don't have a good policy yet, we'll take actions randomly, further, left fasser, further, right fasser further, main engine, do nothing. By just trying out different things in the lunar lander simulator we'll observe a lot of examples of when we're in some state and we took some action, may be a good action, maybe a terrible action either way. Then we got some reward R of s for being in that state, and as a result of our action, we got to some new state, S'. As you take different actions in the lunar lander, you see this S, a, R of s, S', and we call them tuples in Python code many times.\n",
    "\n",
    "Maybe you've done this 10,000 times or even more than 10,000 times, so you would have to save the way with not just S^1, a^1 and so on, but up to S^10,000, a^10,000.\n",
    "\n",
    "When you compute this, this will be some number like 12.5 or 17, or 0.5 or some other number. We'll save that number here as y^1, so that this pair x^1, y^1 becomes the first training example in this low dataset we're computing.\n",
    "\n",
    "But it turns out that when you don't know what is the Q function, you can start off with taking a totally random guess."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/bellman_eq_train_prepar2.png\" width=\"700\"/>\n",
    "\n",
    "We put this over here in our small but growing training set, and so on and so forth, until maybe you end up with 10,000 training examples with these x, y pairs. What we'll see later is that we'll actually take this training set where the x's are inputs with 12 features and the y's are just numbers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Full algorithm for learning the Q-function is like\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/learning_algo.png\" width=\"700\"/>\n",
    "\n",
    "The algorithm you just saw is sometimes called the DQN algorithm which stands for Deep Q-Network because you're using deep learning and neural network to train a model to learn the Q functions. Hence DQN or DQ using a neural network. If you use the algorithm as I described it, it will work, okay, on the lunar lander. Maybe it'll take a long time to converge, maybe it won't land perfectly, but it'll work. But it turns out that with a couple of refinements to the algorithm, it can work much better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/nn_deep_reinforcement.png\" width=\"700\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week3/nn_deep_reinforcement1.png\" width=\"700\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Algorithm refinement: Ïµ-greedy policy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}