{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/movie_rating.png\" width=\"600\"/>\n",
    "\n",
    "From 0 to 5 is movie rating, '?' if user has not seen movie. In the typical recommended system, you have some number of users as well as some number of items."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/movie_rating_2.png\" width=\"600\"/>\n",
    "\n",
    "in this example nm is equal to five because we have five movies. I'm going to set r(i,j)=1, if user j has rated movie i. So for example, use a one Dallas Alice has rated movie one but has not rated movie three and so r(1,1) =1, because she has rated movie one, but r( 3,1)=0 because she has not rated movie number three. Then finally I'm going to use y(i,j). J to denote the rating given by user j to movie i. So for example, this rating here would be that movie three was rated by user 2 to be equal to four."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using per-item features\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/per_item_features.png\" width=\"600\"/>\n",
    "\n",
    "What if we additionally have features of the movies? So here I've added two features X1 and X2, that tell us how much each of these is a romance movie, and how much each of these is an action movie.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/per_item_features2.png\" width=\"600\"/>\n",
    "\n",
    "Where **n** = number of features\n",
    "So you recall that I had used the notation nu to denote the number of users, which is 4 and m to denote the number of movies which is 5. I'm going to also introduce n to denote the number of features we have here. And so n=2, because we have two features X1 and X2 for each movie. With these features we have for example that the features for movie one, that is the movie Love at Last, would be 0.90. And the features for the third movie Cute Puppies of Love would be 0.99 and 0.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/per_item_feature3.png\" width=\"600\"/>\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making recommendations\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/recomendation_system_cost_fun_overfiting.png\" width=\"700\"/>\n",
    "\n",
    "Let me have just one more term to this cost function, which is the regularization term to **prevent overfitting**. And so here's our usual regularization parameter, lambda divided by 2m(j) and then times as sum of the squared values of the parameters w. And so n is a number of numbers in X(i) and that's the same as a number of numbers in w(j). If you were to minimize this cost function J as a function of w and b, you should get a pretty good set of parameters for predicting user j's ratings for other movies. Now, before moving on, it turns out that for recommended systems it would be convenient to actually eliminate this division by m(j) term, m(j) is just a constant in this expression. And so, even if you take it out, you should end up with the same value of w and b."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/cost_fun2.png\" width=\"700\"/>\n",
    "\n",
    "But instead of focusing on a single user, let's look at how we learn the parameters for all of the users. To learn the parameters w(1), b(1), w(2), b(2),...,w(nu), b(nu), we would take this cost function on top and sum it over all the nu users. So we would have sum from j=1 one to nu of the same cost function that we had written up above. And this becomes the cost for learning all the parameters for all of the users. And if we use gradient descent or any other optimization algorithm to minimize this as a function of w(1), b(1) all the way through w(nu), b(nu), then you have a pretty good set of parameters for predicting movie ratings for all the users. And you may notice that this algorithm is a lot like linear regression, where that plays a role similar to the output f(x) of linear regression."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/problem_motivation.png\" width=\"700\"/>\n",
    "\n",
    "if you don't have those features, x_1 and x_2? Let's take a look at how you can learn or come up with those features x_1 and x_2 from the data. Here's the data that we had before. But what if instead of having these numbers for x_1 and x_2, we didn't know in advance what the values of the features x_1 and x_2 were? I'm going to replace them with question marks over here."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/problem_motivation.png\" width=\"700\"/>\n",
    "\n",
    " Given w^1, b^1, w^2, b^2, and so on through w^n_u and b^n_u, for the n subscript u users. If you want to learn the features x^i for a specific movie, i is a cost function we could use which is that. I'm going to want to minimize squared error as usual. If the predicted rating by user j on movie i is given by this, let's take the squared difference from the actual movie rating y,i,j. As before, let's sum over all the users j. But this will be a sum over all values of j, where r, i, j is equal to I. I'll add a 1.5 there as usual. As I defined this as a cost function for x^i. Then if we minimize this as a function of x^i you be choosing the features for movie i. So therefore all the users J that have rated movie i, we will try to minimize the squared difference between what your choice of features x^i results in terms of the predicted movie rating minus the actual movie rating that the user had given it. Then finally, if we want to add a regularization term, we add the usual plus Lambda over 2, K equals 1 through n, where n as usual is the number of features of x^i squared."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/gradient_descent.png\" width=\"700\"/>\n",
    "\n",
    "How do you minimize this cost function as a function of w, b, and x? One thing you could do is to use gradient descent. In course 1 when we learned about linear regression, this is the gradient descent algorithm you had seen, where we had the cost function J, which is a function of the parameters w and b, and we'd apply gradient descent as follows. With collaborative filtering, the cost function is in a function of just w and b is now a function of w, b, and x. I'm using w and b here to denote the parameters for all of the users and x here just informally to denote the features of all of the movies. But if you're able to take partial derivatives with respect to the different parameters, you can then continue to update the parameters as follows. But now we need to optimize this with respect to x as well. We also will want to update each of these parameters x using gradient descent as follows. It turns out that if you do this, then you actually find pretty good values of w and b as well as x. In this formulation of the problem, the parameters of w and b, and x is also a parameter. Then finally, to learn the values of x, we also will update x as x minus the partial derivative respect to x of the cost w, b, x. I'm using the notation here a little bit informally and not keeping very careful track of the superscripts and subscripts, but the key takeaway I hope you have from this is that the parameters to this model are w and b, and x now is also a parameter, which is why we minimize the cost function as a function of all three of these sets of parameters, w and b, as well as x."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/binary_labels.png\" width=\"700\"/>\n",
    "\n",
    "There are many ways of defining what is the label one and what is the label zero, and what is the label question mark in collaborative filtering with binary labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/binary_label_favs_like_cliks.png\" width=\"600\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/q1.png\" width=\"1000\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/q2.png\" width=\"1000\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/q3.png\" width=\"1000\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mean normalization\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/who_not_rated_movies.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/mean_normalization.png\" width=\"700\"/>\n",
    "\n",
    "To carry out mean normalization, what we're going to do is take all of these ratings and for each movie, compute the average rating that was given. So movie one had two 5s and two 0s and so the average rating is 2.5. Movie two had a 5 and a 0, so that averages out to 2.5. Movie three 4 and 0 averages out to 2. Movie four averages out to 2.25 rating. And movie five not that popular, has an average 1.25 rating. So I'm going to take all of these five numbers and gather them into a vector which I'm going to call μ because this is the vector of the average ratings that each of the movies had. Averaging over just the users that did read that particular movie. Instead of using these original 0 to 5 star ratings over here, I'm going to take this and subtract from every rating the mean rating that it was given."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/mean_normalization2.png\" width=\"700\"/>\n",
    "\n",
    "For example this movie rating was 5. I'm going to subtract 2.5 giving me 2.5 over here. This movie had a 0 star rating. I'm going to subtract 2.25 giving me a -2.25 rating and so on for all of the now five users including the new user Eve as well as for all five movies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/mean_normalization3.png\" width=\"700\"/>\n",
    "\n",
    "And using this, you can then learn w(j), b(j) and x(i) same as before for user j on movie i, you would predict w(j).x(i) + b(j)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorFlow implementation of collaborative filtering\n",
    "\n",
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/derivative_in_ml.png\" width=\"700\"/>\n",
    "\n",
    "So the way we were doing that was via a gradient descent update, which looked like this, where w gets repeatedly updated as w minus the learning rate alpha times the derivative term."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/derivative_in_ml2.png\" width=\"700\"/>\n",
    "\n",
    "Sometimes computing this derivative or partial derivative term can be difficult. And it turns out that TensorFlow can help with that. Let's see how."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/3_unsupervised_learning_recommenders_reinforcement_learning/week2/derivative_in_ml3.png\" width=\"700\"/>\n",
    "\n",
    "This is a very powerful feature of TensorFlow called Auto Diff. And some other machine learning packages like pytorch also support Auto Diff. Sometimes you hear people call this Auto Grad. The technically correct term is Auto Diff, and Auto Grad is actually the name of the specific software package for doing automatic differentiation, for taking derivatives automatically."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}