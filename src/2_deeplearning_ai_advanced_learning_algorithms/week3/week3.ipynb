{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Advice for applying machine learning\n",
    "\n",
    "Diagnostic. By diagnostic, I mean a test that you can run to gain insight into what is or isn't working with learning algorithm to gain guidance into improving its performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating a model\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/evaluating_model.png\" width=\"600\"/>\n",
    "\n",
    "So let's take a look at how to evaluate the model. Let's take the example of learning to predict housing prices as a function of the size.\n",
    "\n",
    "This fits the training data really well. But, we don't like this model very much because even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test.png\" width=\"600\"/>\n",
    "\n",
    "So in order to tell if your model is doing well, especially for applications where you have more than one or two features, which makes it difficult to plot f of x. We need some more systematic way to evaluate how well your model is doing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_cost.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_error_cost.png\" width=\"600\"/>\n",
    "\n",
    "So, in the model like what we saw earlier J train of w,b will be low because the average era on your training examples will be zero or very close to zero. So J **train** will be very close to zero. But if you have a few additional examples in your **test** set that the album had not trained on, then those test examples, my love life these. And there's a large gap between what the album is predicting as the estimated housing price"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model selection and training/cross validation/test sets\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_error_cost.png\" width=\"600\"/>\n",
    "\n",
    "The name cross-validation refers to that this is an extra dataset that we're going to use to check or trust check the validity or really the accuracy of different models. I don t think it's a great name, but that is what people in machine learning have gotten to call this extra dataset. You may also hear people call this the validation set for short, it's just fewer syllables than cross-validation or in some applications, people also call this the development set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/cross_val.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/cross_val_formula.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection.png\" width=\"600\"/>\n",
    "\n",
    "??? why VAL set ???\n",
    "you've not fit any parameters, either w or b or d to the test set and that's why Jtest in this example will be **fair estimate** of the generalization error of this model thus parameters w4,b4."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection_NN.png\" width=\"600\"/>\n",
    "\n",
    "If in this example, **second** has the lowest cross validation error, you will then pick the second neural network and use parameters trained on this model and finally, if you want to report out an estimate of the **generalization error**, you then use the **test set** to estimate how well the neural network that you just chose will do.\n",
    "\n",
    " It's considered best practice in machine learning that if you have to make decisions about your model, such as fitting parameters or choosing the model architecture, such as neural network architecture or degree of polynomial if you're fitting a linear regression, to make all those decisions only using your **training set** and your **cross-validation** set, and to **not look at the test set at all while you're still making decisions regarding your learning algorithm.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection_nn2.png\" width=\"600\"/>\n",
    "\n",
    "It's only **after you've come up with one model as your final model** to only then evaluate it on the test set and because you haven't made any decisions using the test set, that **ensures that your test set is a fair** and not **overly optimistic** estimate of how well your model will generalize to new data. That's model selection and this is actually a very widely used procedure. I use this all the time to automatically choose what model to use for a given machine learning application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz1.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz2.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz3.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diagnosing bias and variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regularization and bias/variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion.png\" width=\"700\"/>\n",
    "\n",
    "In this example, I'm going to use a fourth-order polynomial, but we're going to fit this model using regularization. Where here the value of Lambda is the regularization parameter that controls how much you trade-off keeping the parameters w small versus fitting the training data well. Let's start with the example of setting Lambda to be a very large value. Say Lambda is equal to 10,000."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion2.png\" width=\"700\"/>\n",
    "\n",
    "if Lambda were very large, then the algorithm is highly motivated to keep these parameters w very small and so you end up with w_1, w_2, really all of these parameters will be very close to zero. The model ends up being f of x is just approximately b a constant value, which is why you end up with a model like this.\n",
    "This model clearly has **high bias and it underfits** the training data because it doesn't even do well on the training set and J_train is large."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion3.png\" width=\"700\"/>\n",
    "\n",
    "Let's take a look at the other extreme. Let's say you set Lambda to be a very small value. With a small value of Lambda,\n",
    "in fact, let's go to extreme of setting Lambda equals zero. With that choice of Lambda, there is no regularization, so we're just fitting a fourth-order polynomial with no regularization and you end up with that curve that you saw previously that overfits the data. What we saw previously was when you have a model like this, J_train is small, but J_cv is much larger than J_train or J_cv is large. This indicates we have high variance and it overfits this data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion4.png\" width=\"700\"/>\n",
    "\n",
    "if you have some intermediate value of Lambda, not really largely 10,000, but not so small as zero that hopefully you get a model that looks like this, that is just right and fits the data well with small J_train and small J_cv. If you are trying to decide what is a **good value of Lambda to use for the regularization parameter, cross-validation gives you a way to do so as well.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How can you choose a good value of Lambda?\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/choosing_lambda.png\" width=\"700\"/>\n",
    "\n",
    "By trying out a large range of possible values for Lambda, fitting parameters using those different regularization parameters, and then evaluating the performance on the cross-validation set, **you can then try to pick what is the best value for the regularization parameter.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda.png\" width=\"700\"/>\n",
    "\n",
    "you find that J train will go up like this because in the optimization cost function, the **larger Lambda is**, the more the algorithm is trying to keep W squared small. That is, the more weight is given to this regularization term, and thus the less attention is paid to actually do well on the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda2.png\" width=\"700\"/>\n",
    "\n",
    "Now, how about the cross-validation error? Turns out the cross-validation error will look like this. Because we've seen that if Lambda is too small or too large, then it doesn't do well on the cross-validation set. It either overfits here on the left or underfits here on the right."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda3.png\" width=\"700\"/>\n",
    "\n",
    "There'll be some intermediate value of Lambda that causes the algorithm to perform best. What cross-validation is doing is, it's trying out a lot of different values of Lambda. This is what we saw on the last slide; trial Lambda equals zero, Lambda equals 0.01, logic is 0,02. Try a lot of different values of Lambda and evaluate the cross-validation error in a lot of these different points, and then hopefully pick a value that has **low cross validation error**, and this will hopefully **correspond to a good model** for your application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda4.png\" width=\"700\"/>\n",
    "\n",
    "#### High bias (Underfit)\n",
    "#### High variance (Overfit)\n",
    "\n",
    "If you compare this diagram to the one that we had in the previous video, where the horizontal axis was the degree of polynomial, these two diagrams look a little bit not mathematically and not in any formal way, but they look a little bit like mirror images of each other, and that's because when you're fitting a degree of polynomial, the left part of this curve corresponded to underfitting and high bias, the right part corresponded to overfitting and high variance. Whereas in this one, high-variance was on the left and high bias was on the right. But that's why these two images are a little bit like mirror images of each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/baseline_level_of_performance.png\" width=\"700\"/>\n",
    "\n",
    "useful to establish a baseline level of performance, and by baseline level of performance I mean what is the level of error you can reasonably hope your learning algorithm to eventually get to. **One common way to establish a baseline level of performance is to measure how well humans can do on this task** because humans are really good at understanding speech data, or processing images or understanding texts.\n",
    "\n",
    "Another way to estimate a baseline level of performance is if there's some competing algorithm, maybe a previous implementation that someone else has implemented or even a competitor's algorithm to establish a baseline level of performance if you can measure that, or sometimes you might guess based on prior experience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance_example.png\" width=\"700\"/>\n",
    "\n",
    "Just to summarize, this gap between these first two numbers gives you a sense of whether you have a high bias (underfit) problem,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance_example2.png\" width=\"700\"/>\n",
    "\n",
    "and the gap between these two numbers gives you a sense of whether you have a high variance (overfit) problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High bias detection (Underfit)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/hight_bias.png\" width=\"700\"/>\n",
    "\n",
    "If you were to plot the training error, then the training error will go up like so as you'd expect. In fact, this curve of training error may start to flatten out. We call it plateau, meaning flatten out after a while. That's because as you get more and more training examples when you're fitting the simple linear function, your model doesn't actually change that much more. It's fitting a straight line and even as you get more and more and more examples, there's just not that much more to change, which is why the average training error flattens out after a while.\n",
    "\n",
    "Similarly, your cross-validation error will come down and also fattened out after a while, which is why J_cv again is higher than J_train, but J_cv will tend to look like that.\n",
    "\n",
    "If you had a measure of that baseline level of performance, such as human-level performance, then they'll tend to be a value that is lower than your J_train and your J_cv. There's a **big gap between the baseline level** of performance and J_train, which was our ***indicator for this algorithm having high bias.***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/hight_bias_1.png\" width=\"700\"/>\n",
    "\n",
    "if a learning algorithm has high bias, **getting more training data** will not by itself hope that much. I know that we're used to thinking that having more data is good, but if your algorithm has high bias, then if the only thing you do is throw more training data added, that by itself **will not ever let you bring down the error rate** that much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High variance detection (Overfiting)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/high_variance.png\" width=\"700\"/>\n",
    "\n",
    "The fact there's a huge gap here is what I can tell you that this high-variance is doing much better on the training set than it's doing on your cross-validation set.\n",
    "\n",
    "When you're over fitting the training set, you may be able to fit the training set so well to have an unrealistically low error, such as zero error in this example over here, which is actually better than how well humans will actually be able to predict housing prices or whatever the application you're working on.\n",
    "\n",
    "***But again, to signal for high variance is whether J cv is much higher than J train.***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/high_variance1.png\" width=\"700\"/>\n",
    "\n",
    "When you have high variance, then **increasing the training set** size could help a lot.\n",
    "In particular, if we could extrapolate these curves to the right, increase M train, then the training error will continue to go up, but then the cross-validation error hopefully will come down and approach J train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deciding what to try next revisited\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/debugging_learning_algo.png\" width=\"700\"/>\n",
    "\n",
    "- More training example \"fixes high variance 'overfiting'\"\n",
    "- Trying a **smaller set of features**? Sometimes if your learning algorithm has too many features, then it gives your algorithm too much flexibility to fit very complicated models. \"fixes high variance\"\n",
    "- Getting **additional features**, that's just adding additional features is the opposite of going to a smaller set of features. This will help you to fix a high bias problem. \"fixes high bias 'underfiting'\"\n",
    "- Adding polynomial features is a little bit like adding additional features. If you're linear functions, three-line can fit the training set that well, then adding additional polynomial features can help you do better on the training.\n",
    "- Decreasing Lambda means to use a lower value for the regularization parameter. That means we're going to pay less attention to this term and pay more attention to this term to try to do better on the training set.\n",
    "- Finally, increasing Lambda, well that's the opposite of this, but that says you're overfitting the data. Increasing Lambda will make sense if is overfitting the training set,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bias/variance and neural networks\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/tradeoff.png\" width=\"700\"/>\n",
    "\n",
    "One of the reasons that neural networks have been so successful is because your networks, together with the idea of big data or hopefully having large data sets. It's given us a new way of new ways to address both high bias and high variance.\n",
    "\n",
    "\n",
    "- If you were to fit a linear model like this on the left. You have a pretty simple model that can have high bias.\n",
    "- Whereas you were to fit a complex model, then you might suffer from high variance.\n",
    "- Tradeoff between bias and variance, and in our example it was choosing a second order polynomial that helps you make a tradeoff and pick a model with lowest possible cross validation error.\n",
    "\n",
    "you have to balance the complexity that is the degree of polynomial. Or the regularization parameter longer to make buyers and variants both not be too high.\n",
    "\n",
    "But it turns out that your networks offer us away all of this dilemma of having to tradeoff bias and variance with some caveats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/nn_and_regularization.png\" width=\"700\"/>\n",
    "\n",
    "If you have a small neural network, and you were to switch to a much larger neural network, you would think that the risk of overfitting goes up significantly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/nn_and_regularization2.png\" width=\"700\"/>\n",
    "\n",
    "If you want to add regularization then you would just add this extra term colonel regularize A equals l. two and then 0.01 where that's the value of longer in terms of though actually lets you choose different values of lambda for different layers although for simplicity you can choose the same value of lambda for all the weights and all of the different layers as follows. And then this will allow you to implement regularization in your neural network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice quiz: Bias and variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz0.png\" width=\"900\"/>\n",
    "\n",
    "High variance or 'overfiting' mens that model found 'unexisting dependencies on the training data'. As result model can predict 'y' on the training data set extremely accurate 'error very low'. But validation dataset shows normal or high level of errors 'because unseen data'.\n",
    "**This is why high variance or 'overfiting' == big gap between training and validation datasets.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz1.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz2.png\" width=\"900\"/>\n",
    "\n",
    "**additional features** - makes additional properties which model can use for prediction\n",
    "**Small lambda** - customize weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz2_photo.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz3.png\" width=\"900\"/>\n",
    "\n",
    "**high variance or 'overfitting'** - big gap between training and validation errors\n",
    "**more data** - will make data more variative and will reduce unexisting dependency for instance 'second later in the word, len of line etc...'"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}