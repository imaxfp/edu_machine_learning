{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Advice for applying machine learning\n",
    "\n",
    "Diagnostic. By diagnostic, I mean a test that you can run to gain insight into what is or isn't working with learning algorithm to gain guidance into improving its performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating a model\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/evaluating_model.png\" width=\"600\"/>\n",
    "\n",
    "So let's take a look at how to evaluate the model. Let's take the example of learning to predict housing prices as a function of the size.\n",
    "\n",
    "This fits the training data really well. But, we don't like this model very much because even though the model fits the training data well, we think it will fail to generalize to new examples that aren't in the training set.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test.png\" width=\"600\"/>\n",
    "\n",
    "So in order to tell if your model is doing well, especially for applications where you have more than one or two features, which makes it difficult to plot f of x. We need some more systematic way to evaluate how well your model is doing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_cost.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_error_cost.png\" width=\"600\"/>\n",
    "\n",
    "So, in the model like what we saw earlier J train of w,b will be low because the average era on your training examples will be zero or very close to zero. So J **train** will be very close to zero. But if you have a few additional examples in your **test** set that the album had not trained on, then those test examples, my love life these. And there's a large gap between what the album is predicting as the estimated housing price"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model selection and training/cross validation/test sets\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/train_test_error_cost.png\" width=\"600\"/>\n",
    "\n",
    "The name cross-validation refers to that this is an extra dataset that we're going to use to check or trust check the validity or really the accuracy of different models. I don t think it's a great name, but that is what people in machine learning have gotten to call this extra dataset. You may also hear people call this the validation set for short, it's just fewer syllables than cross-validation or in some applications, people also call this the development set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/cross_val.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/cross_val_formula.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection.png\" width=\"600\"/>\n",
    "\n",
    "??? why VAL set ???\n",
    "you've not fit any parameters, either w or b or d to the test set and that's why Jtest in this example will be **fair estimate** of the generalization error of this model thus parameters w4,b4."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection_NN.png\" width=\"600\"/>\n",
    "\n",
    "If in this example, **second** has the lowest cross validation error, you will then pick the second neural network and use parameters trained on this model and finally, if you want to report out an estimate of the **generalization error**, you then use the **test set** to estimate how well the neural network that you just chose will do.\n",
    "\n",
    " It's considered best practice in machine learning that if you have to make decisions about your model, such as fitting parameters or choosing the model architecture, such as neural network architecture or degree of polynomial if you're fitting a linear regression, to make all those decisions only using your **training set** and your **cross-validation** set, and to **not look at the test set at all while you're still making decisions regarding your learning algorithm.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/model_selection_nn2.png\" width=\"600\"/>\n",
    "\n",
    "It's only **after you've come up with one model as your final model** to only then evaluate it on the test set and because you haven't made any decisions using the test set, that **ensures that your test set is a fair** and not **overly optimistic** estimate of how well your model will generalize to new data. That's model selection and this is actually a very widely used procedure. I use this all the time to automatically choose what model to use for a given machine learning application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz1.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz2.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz3.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diagnosing bias and variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regularization and bias/variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion.png\" width=\"700\"/>\n",
    "\n",
    "In this example, I'm going to use a fourth-order polynomial, but we're going to fit this model using regularization. Where here the value of Lambda is the regularization parameter that controls how much you trade-off keeping the parameters w small versus fitting the training data well. Let's start with the example of setting Lambda to be a very large value. Say Lambda is equal to 10,000."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion2.png\" width=\"700\"/>\n",
    "\n",
    "if Lambda were very large, then the algorithm is highly motivated to keep these parameters w very small and so you end up with w_1, w_2, really all of these parameters will be very close to zero. The model ends up being f of x is just approximately b a constant value, which is why you end up with a model like this.\n",
    "This model clearly has **high bias and it underfits** the training data because it doesn't even do well on the training set and J_train is large."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion3.png\" width=\"700\"/>\n",
    "\n",
    "Let's take a look at the other extreme. Let's say you set Lambda to be a very small value. With a small value of Lambda,\n",
    "in fact, let's go to extreme of setting Lambda equals zero. With that choice of Lambda, there is no regularization, so we're just fitting a fourth-order polynomial with no regularization and you end up with that curve that you saw previously that overfits the data. What we saw previously was when you have a model like this, J_train is small, but J_cv is much larger than J_train or J_cv is large. This indicates we have high variance and it overfits this data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/lambda_regularizarion4.png\" width=\"700\"/>\n",
    "\n",
    "if you have some intermediate value of Lambda, not really largely 10,000, but not so small as zero that hopefully you get a model that looks like this, that is just right and fits the data well with small J_train and small J_cv. If you are trying to decide what is a **good value of Lambda to use for the regularization parameter, cross-validation gives you a way to do so as well.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How can you choose a good value of Lambda?\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/choosing_lambda.png\" width=\"700\"/>\n",
    "\n",
    "By trying out a large range of possible values for Lambda, fitting parameters using those different regularization parameters, and then evaluating the performance on the cross-validation set, **you can then try to pick what is the best value for the regularization parameter.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda.png\" width=\"700\"/>\n",
    "\n",
    "you find that J train will go up like this because in the optimization cost function, the **larger Lambda is**, the more the algorithm is trying to keep W squared small. That is, the more weight is given to this regularization term, and thus the less attention is paid to actually do well on the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda2.png\" width=\"700\"/>\n",
    "\n",
    "Now, how about the cross-validation error? Turns out the cross-validation error will look like this. Because we've seen that if Lambda is too small or too large, then it doesn't do well on the cross-validation set. It either overfits here on the left or underfits here on the right."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda3.png\" width=\"700\"/>\n",
    "\n",
    "There'll be some intermediate value of Lambda that causes the algorithm to perform best. What cross-validation is doing is, it's trying out a lot of different values of Lambda. This is what we saw on the last slide; trial Lambda equals zero, Lambda equals 0.01, logic is 0,02. Try a lot of different values of Lambda and evaluate the cross-validation error in a lot of these different points, and then hopefully pick a value that has **low cross validation error**, and this will hopefully **correspond to a good model** for your application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_large_lambda4.png\" width=\"700\"/>\n",
    "\n",
    "#### High bias (Underfit)\n",
    "#### High variance (Overfit)\n",
    "\n",
    "If you compare this diagram to the one that we had in the previous video, where the horizontal axis was the degree of polynomial, these two diagrams look a little bit not mathematically and not in any formal way, but they look a little bit like mirror images of each other, and that's because when you're fitting a degree of polynomial, the left part of this curve corresponded to underfitting and high bias, the right part corresponded to overfitting and high variance. Whereas in this one, high-variance was on the left and high bias was on the right. But that's why these two images are a little bit like mirror images of each other."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/baseline_level_of_performance.png\" width=\"700\"/>\n",
    "\n",
    "useful to establish a baseline level of performance, and by baseline level of performance I mean what is the level of error you can reasonably hope your learning algorithm to eventually get to. **One common way to establish a baseline level of performance is to measure how well humans can do on this task** because humans are really good at understanding speech data, or processing images or understanding texts.\n",
    "\n",
    "Another way to estimate a baseline level of performance is if there's some competing algorithm, maybe a previous implementation that someone else has implemented or even a competitor's algorithm to establish a baseline level of performance if you can measure that, or sometimes you might guess based on prior experience."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance_example.png\" width=\"700\"/>\n",
    "\n",
    "Just to summarize, this gap between these first two numbers gives you a sense of whether you have a high bias (underfit) problem,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias_variance_example2.png\" width=\"700\"/>\n",
    "\n",
    "and the gap between these two numbers gives you a sense of whether you have a high variance (overfit) problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High bias detection (Underfit)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/hight_bias.png\" width=\"700\"/>\n",
    "\n",
    "If you were to plot the training error, then the training error will go up like so as you'd expect. In fact, this curve of training error may start to flatten out. We call it plateau, meaning flatten out after a while. That's because as you get more and more training examples when you're fitting the simple linear function, your model doesn't actually change that much more. It's fitting a straight line and even as you get more and more and more examples, there's just not that much more to change, which is why the average training error flattens out after a while.\n",
    "\n",
    "Similarly, your cross-validation error will come down and also fattened out after a while, which is why J_cv again is higher than J_train, but J_cv will tend to look like that.\n",
    "\n",
    "If you had a measure of that baseline level of performance, such as human-level performance, then they'll tend to be a value that is lower than your J_train and your J_cv. There's a **big gap between the baseline level** of performance and J_train, which was our ***indicator for this algorithm having high bias.***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/hight_bias_1.png\" width=\"700\"/>\n",
    "\n",
    "if a learning algorithm has high bias, **getting more training data** will not by itself hope that much. I know that we're used to thinking that having more data is good, but if your algorithm has high bias, then if the only thing you do is throw more training data added, that by itself **will not ever let you bring down the error rate** that much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### High variance detection (Overfiting)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/high_variance.png\" width=\"700\"/>\n",
    "\n",
    "The fact there's a huge gap here is what I can tell you that this high-variance is doing much better on the training set than it's doing on your cross-validation set.\n",
    "\n",
    "When you're over fitting the training set, you may be able to fit the training set so well to have an unrealistically low error, such as zero error in this example over here, which is actually better than how well humans will actually be able to predict housing prices or whatever the application you're working on.\n",
    "\n",
    "***But again, to signal for high variance is whether J cv is much higher than J train.***\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/high_variance1.png\" width=\"700\"/>\n",
    "\n",
    "When you have high variance, then **increasing the training set** size could help a lot.\n",
    "In particular, if we could extrapolate these curves to the right, increase M train, then the training error will continue to go up, but then the cross-validation error hopefully will come down and approach J train."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deciding what to try next revisited\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/debugging_learning_algo.png\" width=\"700\"/>\n",
    "\n",
    "- More training example \"fixes high variance 'overfiting'\"\n",
    "- Trying a **smaller set of features**? Sometimes if your learning algorithm has too many features, then it gives your algorithm too much flexibility to fit very complicated models. \"fixes high variance\"\n",
    "- Getting **additional features**, that's just adding additional features is the opposite of going to a smaller set of features. This will help you to fix a high bias problem. \"fixes high bias 'underfiting'\"\n",
    "- Adding polynomial features is a little bit like adding additional features. If you're linear functions, three-line can fit the training set that well, then adding additional polynomial features can help you do better on the training.\n",
    "- Decreasing Lambda means to use a lower value for the regularization parameter. That means we're going to pay less attention to this term and pay more attention to this term to try to do better on the training set.\n",
    "- Finally, increasing Lambda, well that's the opposite of this, but that says you're overfitting the data. Increasing Lambda will make sense if is overfitting the training set,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bias/variance and neural networks\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/tradeoff.png\" width=\"700\"/>\n",
    "\n",
    "One of the reasons that neural networks have been so successful is because your networks, together with the idea of big data or hopefully having large data sets. It's given us a new way of new ways to address both high bias and high variance.\n",
    "\n",
    "\n",
    "- If you were to fit a linear model like this on the left. You have a pretty simple model that can have high bias.\n",
    "- Whereas you were to fit a complex model, then you might suffer from high variance.\n",
    "- Tradeoff between bias and variance, and in our example it was choosing a second order polynomial that helps you make a tradeoff and pick a model with lowest possible cross validation error.\n",
    "\n",
    "you have to balance the complexity that is the degree of polynomial. Or the regularization parameter longer to make buyers and variants both not be too high.\n",
    "\n",
    "But it turns out that your networks offer us away all of this dilemma of having to tradeoff bias and variance with some caveats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/nn_and_regularization.png\" width=\"700\"/>\n",
    "\n",
    "If you have a small neural network, and you were to switch to a much larger neural network, you would think that the risk of overfitting goes up significantly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/nn_and_regularization2.png\" width=\"700\"/>\n",
    "\n",
    "If you want to add regularization then you would just add this extra term colonel regularize A equals l. two and then 0.01 where that's the value of longer in terms of though actually lets you choose different values of lambda for different layers although for simplicity you can choose the same value of lambda for all the weights and all of the different layers as follows. And then this will allow you to implement regularization in your neural network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice quiz: Bias and variance\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz0.png\" width=\"900\"/>\n",
    "\n",
    "High variance or 'overfiting' mens that model found 'unexisting dependencies on the training data'. As result model can predict 'y' on the training data set extremely accurate 'error very low'. But validation dataset shows normal or high level of errors 'because unseen data'.\n",
    "**This is why high variance or 'overfiting' == big gap between training and validation datasets.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz1.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz2.png\" width=\"900\"/>\n",
    "\n",
    "**additional features** - makes additional properties which model can use for prediction\n",
    "**Small lambda** - customize weight"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz2_photo.png\" width=\"900\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3_practice/quiz3.png\" width=\"900\"/>\n",
    "\n",
    "**high variance or 'overfitting'** - big gap between training and validation errors\n",
    "**more data** - will make data more variative and will reduce unexisting dependency for instance 'second later in the word, len of line etc...'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterative loop of ML development\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/iterative_loop_dev.png\" width=\"700\"/>\n",
    "\n",
    "- Choosing your machine learning model as well as deciding what data to use, maybe picking the hyperparameters, and so on\n",
    "- Implement and train a model. As I've mentioned before, when you train a model for the first time, it will almost never work as well as you want it to.\n",
    "- Implement or to look at a few diagnostics, such as looking at the bias and variance of your algorithm as well as something we'll see in the next video called error analysis.\n",
    "- Based on the insights from the diagnostics, you can then make decisions like do want to make your neural network bigger or change the Lambda regularization parameter, or maybe add more data or add more features or subtract features.\n",
    "- Then you go around this loop again with your new choice of architecture, and it will often take multiple iterations through this loop until you get to the performance that you want."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error analysis\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/dev_proc_spam_classifier_exalple.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Adding data, creating more data\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/adding_data.png\" width=\"700\"/>\n",
    "\n",
    "technique that's why they use especially for images and audio data that can increase your training set size significantly. This technique is called **data augmentation**. And what we're going to do is take an existing train example to create a new training example. For example if you're trying to recognize the letters from A to Z for an [INAUDIBLE] optical character recognition problem. So not just the digits 0-9 but also the letters from A to Z. Given an image like this, you might decide to create a new training example by **rotating the image** a bit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/data_augmentation.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "Or by enlarging the image a bit or by shrinking a little bit or by changing the contrast of the image. And these are examples of distortions to the image that don't change the fact that this is still the letter A And for some letters but not others you can also take the mirror image of the letter and it still looks like the letter A."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/data_augmentation2.png\" width=\"700\"/>\n",
    "\n",
    "More advanced example of data augmentation. You can also take the letter A and place a grid on top of it. And by introducing random warping of this grid, you can take the letter A. And introduce war pings of the leather A to create a much richer library of examples of the letter A."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## !Transfer learning\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/transfer_learning1.png\" width=\"700\"/>\n",
    "\n",
    "Let's take a look at how transfer learning works.\n",
    "\n",
    "You want to recognize the handwritten digits from zero through nine but you don't have that much labeled data of these handwritten digits.\n",
    "\n",
    "Say you find a very large datasets of one million images of pictures of cats, dogs, cars, people, and so on, a thousand classes. You can then start by training a neural network on this large dataset of a million images with a thousand different classes and train the algorithm to take as input an image X, and learn to recognize any of these 1,000 different classes.\n",
    "\n",
    "In this process, you end up learning parameters for the first layer of the neural network W^1, b^1, for the second layer W^2, b^2, and so on...\n",
    "\n",
    "But for the last layer, you would eliminate the output layer and replace it with a much smaller output layer with just 10 rather than 1,000 output units.\n",
    "\n",
    "This algorithm is called transfer learning because the intuition is by learning to recognize cats, dogs, cows, people, and so on. It will hopefully, have learned some plausible sets of parameters for the earlier layers for processing image inputs. Then by transferring these parameters to the new neural network, the new neural network starts off with the parameters in a much better place so that we have just a little bit of further learning.\n",
    "\n",
    " Then the second step is called fine tuning where you take the parameters that you had initialized or gotten from supervised pre-training and then run gradient descent further to fine tune the weights to suit the specific application of handwritten digit recognition that you may have\n",
    "\n",
    "Downloading a pre-trained model that someone else has trained and provided for free is one of those techniques where by building on each other's work on machine learning community we can all get much better results. By the generosity of other researchers that have pre-trained and posted their neural networks online."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/transfer_learning2.png\" width=\"700\"/>\n",
    "\n",
    "Each of these squares is a visualization of what a single neuron has learned to detect as learn to group together pixels to find edges in an image.\n",
    "\n",
    "The next layer of the neural network then learns to group together edges to **detect corners**. Each of these is a visualization of what one neuron may have learned to detect, must **learn to technical, simple shapes like corner like shapes**.\n",
    "\n",
    "The **next layer** of the neural network may have learned to **detect some are more complex**, they store generic shapes like basic curves or smaller shapes like these. That's why by **learning on detecting lots of different images**, you're teaching the neural network to **detect edges, corners, and basic shapes.** That's why by training a neural network to **detect things as diverse as cats, dogs, cars and people, you're helping it to learn to detect these pretty generic features of images and finding edges, corners, curves, basic shapes.**\n",
    "\n",
    "This is **useful for many other computer vision tasks**, such as **recognizing handwritten digits.** One restriction of pre-training though, is that the image type x has to be the same for the pre-training and fine-tuning steps. If the final task you want to solve is a computer vision tasks, then the pre-training step also has been a neural network trained on the same type of input, namely an image of the desired dimensions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/transfer_learning_summ.png\" width=\"700\"/>\n",
    "\n",
    "Those have been successful applications of pre-training in the machine learning literature. One of the things I like about transfer learning is just that one of the ways that the machine learning community has shared ideas, and code, and even parameters, with each other because thanks to the researchers that have pre-trained large neural networks and posted the parameters on the internet freely for anyone else to download and use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Full cycle of a machine learning project\n",
    "\n",
    "Building a machine learning system I find that training a model is just part of the puzzle.\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/full_cycle_ml_project.png\" width=\"700\"/>\n",
    "\n",
    "- first step of machine learning project is to **scope the project**. In other words, decide **what is the project** and what you want to work on. For example, I once decided to work on speech recognition for voice search.\n",
    "- Decide **what data you need to train your machine learning system** and go and do the work to get the audio and get the transcripts of the labels for your dataset. That's **data collection.**\n",
    "- After you have your initial data collection you can then **start to train the model.** Here you would train a speech recognition system and caravel error analysis and iteratively improve your model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/full_cycle_ml_project2.png\" width=\"700\"/>\n",
    "\n",
    "- After you've started training the model for error analysis or for a bias-variance analysis to tell you that you might want to go back to collect more data. Maybe collect more data of everything or just collect more data of a specific type where your error analysis tells you you want to improve the performance of your learning algorithm. For example, once when working on speech I realized that my model was doing particularly poorly when **there was car noise in the background.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/full_cycle_ml_project3.png\" width=\"700\"/>\n",
    "\n",
    "- When you deploy a system you have to also make sure that you continue to monitor the performance of the system and to maintain the system in case the performance gets worse to bring us performance back up instead of just hosting your machine learning model on a server. I'll say a little bit more about why you need to maintain these machine learning systems on the next slide."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/full_cycle_ml_project4.png\" width=\"700\"/>\n",
    "\n",
    "But after this deployment, sometimes you realize that is not working as well as you hoped and you go back to train the model to improve it again or even go back and get more data. In fact, if users and if you have permission to use data from your production deployment, sometimes that data from your working speech system can give you access to even more data with which to keep on improving the performance of your system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ML and Fairness, bias, and ethics\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/bias.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/disease_classification.png\" width=\"700\"/>\n",
    "\n",
    " Specifically, if it is a rare disease and if only 0.5 percent of the patients in your population have the disease, then if instead you wrote the program, that just said, **print y equals 0**. It **predicts y equals 0 all the time**. This very simple even non-learning algorithm, because it just says y equals 0 all the time, this will actually have 99.5 percent accuracy or 0.5 percent error. This really dumb algorithm outperforms your learning algorithm which had one percent error, much worse than 0.5 percent error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz01.png\" width=\"800\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/quiz02.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/disease_classification.png\" width=\"800\"/>\n",
    "\n",
    " Specifically, if it is a rare disease and if only 0.5 percent of the patients in your population have the disease, then if instead you wrote the program, that just said, print y equals 0. It predicts y equals 0 all the time. This very simple even non-learning algorithm, because it just says y equals 0 all the time, this will actually have 99.5 percent accuracy or 0.5 percent error. This really dumb algorithm outperforms your learning algorithm which had one percent error, much worse than 0.5 percent error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/precision.png\" width=\"800\"/>\n",
    "\n",
    "The precision of the learning algorithm computes of all the patients where we predicted y is equal to 1, what fraction actually has the rare disease.\n",
    "\n",
    "In other words, precision is defined as the number of true positives divided by the number classified as positive. In other words, of all the examples you predicted as positive, what fraction did we actually get right.\n",
    "\n",
    " this formula would be true positives divided by true positives plus false positives because it is by summing this cell and this cell that you end up with the total number that was predicted as positive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/recall.png\" width=\"800\"/>\n",
    "\n",
    "The second metric that is useful to compute is recall. And recall asks: Of all the patients that actually have the rare disease, what fraction did we correctly detect as having it? Recall is defined as the number of true positives divided by the number of actual positives."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week3/precision_recall.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "In general, a learning algorithm with either zero precision or zero recall is not a useful algorithm. But just as a side note, if an algorithm actually predicts zero all the time, precision actually becomes undefined because it's actually zero over. zero. But in practice, if an algorithm doesn't predict even a single positive, we just say that precision is also equal to zero. But we'll find that computing both precision and recall makes it easier to spot if an algorithm is both reasonably accurate, in that, when it says a patient has a disease, there's a good chance the patient has a disease, such as 0.75 chance in this example, and also making sure that of all the patients that have the disease, it's helping to diagnose a reasonable fraction of them, such as here it's finding 60 percent of them. When you have a rare class, looking at precision and recall and making sure that both numbers are decently high, that hopefully helps reassure you that your learning algorithm is actually useful. The term recall was motivated by this observation that if you have a group of patients or population of patients, then recall measures, of all the patients that have the disease, how many would you have accurately diagnosed as having it. So when you have skewed classes or a rare class that you want to detect, precision and recall helps you tell if your learning algorithm is making good predictions or useful predictions. Now that we have these metrics for telling how well your learning algorithm is doing, in the next video, let's take a look at how to trade-off between precision and recall to try to optimize the performance of your learning algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}