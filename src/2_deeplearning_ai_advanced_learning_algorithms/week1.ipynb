{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Week 1\n",
    "\n",
    "## Neurons and the brain\n",
    "Artificial neural network uses a very simplified Mathematical model of what a biological neuron does.\n",
    "What these neurons do collectively is input a few numbers, carry out some computation, and output some other numbers.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/neurons_example.png\" width=\"500\"/>\n",
    "\n",
    "### Demand Prediction\n",
    "\n",
    "Example for demand prediction in which you look at the product and try to predict, will this product be a top seller or not?\n",
    "\n",
    "Previously, we had written this as f of x as the output of the learning algorithm. In order to set us up to build a neural network, I'm going to switch the terminology a little bit and use the alphabet a to denote the output of this **logistic regression algorithm**. The term a stands for activation, and it's actually a term from neuroscience, and it refers to how much a neuron is sending a high output to other neurons downstream from it.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction.png\" width=\"500\"/>\n",
    "\n",
    "^*\n",
    "**It turns out that this logistic regression units or this little logistic regression algorithm, can be thought of as a very simplified model of a single neuron in the brain.**\n",
    "\n",
    "Where what the neuron does is it takes us input the price x, and then it computes this formula on top, and it outputs the number a, which is computed by this formula, and it outputs the probability of this T-shirt being a top seller.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction2.png\" width=\"500\"/>\n",
    "\n",
    "First: I'm going to do is create one artificial neuron to try to estimate the probability that this T-shirt is perceive as highly **affordable**. Logistic regression unit to input price and shipping costs and predict do people think this is affordable?\n",
    "\n",
    "Second: I'm going to create another artificial neuron here to estimate, is there high **awareness** of this? Awareness in this case is mainly a function of the marketing of the T-shirt.\n",
    "\n",
    "Finally: going to create another neuron to estimate do people **perceive** this to be of high quality, and that may mainly be a function of the price of the T-shirt and of the material quality.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction3.png\" width=\"500\"/>\n",
    "\n",
    "Given these estimates of affordability, awareness, and perceived quality we then wire the outputs of these three neurons to another neuron here on the right, that then there's another logistic regression unit. That finally inputs those three numbers and outputs the probability of this t-shirt being a top seller.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction4.png\" width=\"500\"/>\n",
    "\n",
    "In the terminology of neural networks, we're going to group these three neurons together into what's called a layer. A layer is a grouping of neurons which takes as input the same or similar features, and that in turn outputs a few numbers together. These three neurons on the left form one layer which is why I drew them on top of each other, and this single neuron on the right is also one layer. The layer on the left has three neurons, so a layer can have multiple neurons or it can also have a single neuron as in the case of this layer on the right.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction5.png\" width=\"500\"/>\n",
    "\n",
    "This layer on the right is also called the output layer because the outputs of this final neuron is the output probability predicted by the neural network. In the terminology of neural networks we're also going to call affordability awareness and perceive quality to be activations. The term activations comes from biological neurons, and it refers to the degree that the biological neuron is sending a high output value or sending many electrical impulses to other neurons to the downstream from it.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction6.png\" width=\"500\"/>\n",
    "\n",
    "This particular neural network therefore carries out computations as follows. It inputs four numbers then this layer of the neural network uses those four numbers to compute the new numbers also called activation values. Then the final layer, the output layer of the neural network used those three numbers to compute one number. In a neural network this list of four numbers is also called the input layer, and that's just a list of four numbers. Now, there's one simplification I'd like make to this neural network.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction7.png\" width=\"500\"/>\n",
    "\n",
    "if you're building a large neural network it'd be a lot of work to go through and manually decide which neurons should take **which features as inputs**. The way a neural network is implemented in practice **each neuron** in a certain layer; say this layer in the middle, will have **access to every feature**, to every value from the previous layer\n",
    "\n",
    "You can imagine that if you're trying to **predict affordability** and it knows what's the price shipping cost **marketing and material**, may be you'll learn to **ignore marketing and material** and just figure out through setting the parameters appropriately to only focus on the subset of features that are **most relevant to affordability.**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction8.png\" width=\"500\"/>\n",
    "\n",
    "To further simplify the notation and the description of this neural network I'm going to take these four input features and write them as a vector x, and we're going to view the neural network as having four features that comprise this feature vector x. This feature vector is fed to this layer in the middle which then computes three activation values. That is these numbers and these three activation values in turn becomes another vector which is fed to this final output layer that finally outputs the probability of this t-shirt to being a top seller. That's all a neural network is. It has a few layers where each layer inputs a vector and outputs another vector of numbers. For example, this layer in the middle inputs four numbers x and outputs three numbers corresponding to affordability, awareness, and perceived quality. To add a little bit more terminology, you've seen that this layer is called the output layer and this layer is called the input layer. Your data set tells you what is x and what is y, and so you get data that tells you what are the correct inputs and the correct outputs. But your dataset doesn't tell you what are the correct values for affordability, awareness, and perceived quality. The correct values for those are hidden. You don't see them in the training set, which is why this layer in the middle is called a hidden layer.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/demand_prediction9.png\" width=\"500\"/>\n",
    "\n",
    "To summarize, a neural network, does this, the input layer has a vector of features, four numbers in this example, it is input to the hidden layer, which outputs three numbers. I'm going to use a vector to denote this vector of activations that this hidden layer outputs. Then the output layer takes its input to three numbers and outputs one number, which would be the final activation, or the final prediction of the neural network. One note, even though I previously described this neural network as computing affordability, awareness, and perceived quality, one of the really nice properties of a neural network is when you train it from data, you don't need to go in to explicitly decide what other features, such as affordability and so on, that the neural network should compute instead or figure out all by itself what are the features it wants to use in this hidden layer.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example: Recognizing Images\n",
    "\n",
    "\n",
    "If you were to take these pixel intensity values and unroll them into a vector, you end up with a list or a vector of a million pixel intensity values. One million because 1,000 by 1,000 square gives you a million numbers.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/face_recognition1.png\" width=\"500\"/>\n",
    "\n",
    "Can you train a neural network that takes as input a feature vector with a million pixel brightness values and outputs the identity of the person in the picture?\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/face_recognition2.png\" width=\"500\"/>\n",
    "\n",
    " It turns out that when you train a system like this on a lot of pictures of faces and you peer at the different neurons in the hidden layers to figure out what they may be computing this is what you might find. In the first hidden layer, you might find one neuron that is looking for the low vertical line or a vertical edge like that. A second neuron looking for a oriented line or oriented edge like that. The third neuron looking for a line at that orientation, and so on.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/face_recognition3.png\" width=\"500\"/>\n",
    "\n",
    "In the earliest layers of a neural network, you might find that the neurons are looking for very short lines or very short edges in the image. If you look at the next hidden layer, you find that these neurons might learn to group together lots of little short lines and little short edge segments in order to look for parts of faces. For example, each of these little square boxes is a visualization of what that neuron is trying to detect. This first neuron looks like it's trying to detect the presence or absence of an eye in a certain position of the image. The second neuron, looks like it's trying to detect like a corner of a nose and maybe this neuron over here is trying to detect the bottom of an ear. Then as you look at the next hidden layer in this example, the neural network is aggregating different parts of faces to then try to detect presence or absence of larger, coarser face shapes. Then finally, detecting how much the face corresponds to different face shapes creates a rich set of features that then helps the output layer try to determine the identity of the person picture. A remarkable thing about the neural network is you can learn these feature detectors at the different hidden layers all by itself. In this example, no one ever told it to look for short little edges in the first layer, and eyes and noses and face parts in the second layer and then more complete face shapes at the third layer. The neural network is able to figure out these things all by itself from data.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/face_recognition4.png\" width=\"500\"/>\n",
    "\n",
    "The same learning algorithm is asked to detect cars, will then learn edges in the first layer. Pretty similar but then they'll learn to detect parts of cars in the second hidden layer and then more complete car shapes in the third hidden layer. Just by feeding it different data, the neural network automatically learns to detect very different features so as to try to make the predictions of car detection or person recognition or whether there's a particular given task that is trained on. That's how a neural network works for computer vision application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural network layer\n",
    "Let's take a look at how a layer of neurons works.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/nn_layer1.png\" width=\"500\"/>\n",
    "\n",
    "The **first neuron** ends up being a number 0.3 and that's the activation value a of the first neuron.\n",
    "\n",
    "The **second neuron** has parameters w_2 and b_2, and these w, b or w_2, b_2 are the parameters of the second logistic unit. It computes a_2 equals the logistic function g applied to w_2 dot product x plus b_2 and this may be some other number, say 0.7. Because in this example, there's a 0.7 chance that we think the potential buyers will be aware of this t-shirt.\n",
    "\n",
    "Similarly, it computes an activation value a_3 equals g of w_3 dot product x plus b_3 and that may be say, 0.2.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/nn_layer2.png\" width=\"500\"/>\n",
    "\n",
    "This vector of three numbers becomes the vector of activation values a, that is then passed to the final output layer of this neural network.\n",
    "\n",
    "when you build neural networks with multiple layers, it'll be useful to give the layers different numbers. By convention, this layer is called layer 1 of the neural network and this layer is called layer 2 of the neural network. The input layer is also sometimes called layer 0\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/nn_layer3.png\" width=\"500\"/>\n",
    "\n",
    "Now let's zoom into the computation of layer 2 of this neural network, which is also the output layer. The input to layer 2 is the output of layer 1, so a_1 is this vector 0.3, 0.7, 0.2 that we just computed on the previous part of this slide. Because the output layer has just a single neuron, all it does is it computes a_1 that is the output of this first and only neuron, as g, the sigmoid function applied to w _1 in a product with a^[1], so this is the input into this layer, and then plus b_1. Here, this is the quantity z that you familiar with and g as before is the sigmoid function that you apply to this. If this results in a number, say 0.84, then that becomes the output layer of the neural network. In this example, because the output layer has just a single neuron, this output is just a scalar, is a single number rather than a vector of numbers.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/nn_layer4.png\" width=\"500\"/>\n",
    "\n",
    "so a^[2] is the output of this layer, and so I'm going to also copy this here as the final output of the neural network.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/nn_layer5.png\" width=\"500\"/>\n",
    "\n",
    " Once the neural network has computed a_2, there's one final optional step that you can choose to implement or not, which is if you want a binary prediction, 1 or 0, is this a top seller? Yes or no? As you can take the number a superscript square brackets 2 subscript 1, and this is the number 0.84 that we computed, and threshold this at 0.5. If it's greater than 0.5, you can predict y hat equals 1 and if it is less than 0.5, then predict your y hat equals 0.\n",
    "\n",
    "So that's how a neural network works. Every layer inputs a vector of numbers and applies a bunch of logistic regression units to it, and then computes another vector of numbers that then gets passed from layer to layer until you get to the final output layers computation, which is the prediction of the neural network. Then you can either threshold at 0.5 or not to come up with the final prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### More complex neural networks\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/more_complex_nn1.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/more_complex_nn2.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/more_complex_nn3.png\" width=\"500\"/>\n",
    "\n",
    "The activation of the 2nd neuron at layer 3 is denoted by 'a' three two. To apply the activation function, g, lets use the parameters of this same neuron. So w and b will have the same subscript 2 and superscript square bracket 3. The input features will be the output vector from the previous layer, which is layer 2. So that will be the vector 'a' superscript 2. The second option is using vector ‘a’ 3 which is not the output vector from the previous layer. The input to this layer is 'a' two. And the 3rd option has a two two as input, which is a single number rather than the vector Because recall that the correct input is a vector, a two, with the little arrow on top, and not a single number.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/more_complex_nn4.png\" width=\"500\"/>\n",
    "\n",
    "Main formula"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### making predictions (forward propagation)\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/handwriten_digit_recognition1.png\" width=\"500\"/>\n",
    "\n",
    "Given these 64 input features, we're going to use the neural network with two hidden layers\n",
    "\n",
    "let's step through the sequence of computations that in your neural network will need to make to go from the input X, this eight by eight or 64 numbers to the predicted probability a3. The first computation is to go from X to a1, and that's what the first layer of the first hidden layer does.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/handwritten_digit_recognition2.png\" width=\"500\"/>\n",
    "\n",
    " Notice that a one has 25 numbers because this hidden layer has 25 units. Which is why the parameters go from w1 through w25 as well as b1 through b25. And I've written x here but I could also have written a0 here because by convention the activation of layer zero, that is a0 is equal to the input feature value x. So let's just compute a1. The next step is to compute a2. Looking at the second hidden layer, it then carries out this computation where a2 is a function of a1 and it's computed as the safe point activation function applied to w dot product a1 plus the corresponding value of b.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/handwritten_digit_recognition3.png\" width=\"500\"/>\n",
    "\n",
    "Notice that layer two has 15 neurons or 15 units, which is why the parameters Here run from w1 through w15 and b1 through b15. Now we've computed a2. The Final step is then to compute a3 and we do so using a very similar computation.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/handwritten_digit_recognition4.png\" width=\"500\"/>\n",
    "\n",
    "The Final step is then to compute a3 and we do so using a very similar computation. Only now, this third layer, the output layer has just one unit, which is why there's just one output here. So a3 is just a scalar. And finally you can optionally take a3 subscript one and threshold it at 4.5 to come up with a binary classification label. Is this the digit 1? Yes or no?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regression/Linear Model\n",
    "The function implemented by a neuron with no activation is the same as in Course 1, linear regression:\n",
    "$$ f_{\\mathbf{w},b}(x^{(i)}) = \\mathbf{w}\\cdot x^{(i)} + b \\tag{1}$$\n",
    "\n",
    "We can define a layer with one neuron or unit and compare it to the familiar linear regression function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Questions:\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/layer_notations.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"../../img/2_advanced_learning_algorithms/nn/output_layer.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "1.21.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from lab_utils_common import dlc\n",
    "from lab_neurons_utils import plt_prob_1d, sigmoidnp, plt_linear, plt_logistic\n",
    "plt.style.use('./deeplearning.mplstyle')\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}