{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_example2.png\" width=\"700\"/>\n",
    "\n",
    "Obviously in real life our data will not be this clean but the logic that a decision tree employs remains the same. At each node, it will ask —\n",
    "What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification with categorical (discrete values)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/binary_classification_categorical.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree.png\" width=\"700\"/>\n",
    "\n",
    "What is a decision tree? Here's an example of a model that you might get after training a decision tree learning algorithm on the data set that you just saw. The model that is output by the learning algorithm looks like a tree, and a picture like this is what computer scientists call a tree.\n",
    "\n",
    "Based on the value of the ear shape of this example we'll either go left or go right. The value of the ear-shape with this example is pointy, and so we'll go down the left branch of the tree, like so, and end up at this oval node over here. We then look at the face shape of this example, which turns out to be round, and so we will follow this arrow down over here. The algorithm will make a inference that it thinks this is a cat."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree2.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "The first step of decision tree learning is, we have to decide what feature to use at the root node.\n",
    "\n",
    "What that means is we will decide to look at all of our training examples, all tangent examples shown here, I split them according to the value of the ear shape feature.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning2.png\" width=\"700\"/>\n",
    "\n",
    "The second step is focusing just on the left part or sometimes called the left branch of the decision tree to decide what nodes to put over there. In particular, what feature that we want to split on or what feature do we want to use next.\n",
    "\n",
    "What we'll do now is take these five examples and split these five examples into two subsets based on their value of the face shape."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning3.png\" width=\"700\"/>\n",
    "\n",
    "Having done this on the left part to the left branch of this decision tree, we now repeat a similar process on the right part or the right branch of this decision tree. Focus attention on just these five examples, which contains one captain for dogs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree. Make Decision 1\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning4.png\" width=\"700\"/>\n",
    "\n",
    "how do you choose what features to use to split on at each node? At the root node, as well as on the left branch and the right branch of the decision tree, we had to decide if there were a few examples at that node comprising a mix of cats and dogs.\n",
    "\n",
    "The decision tree learning algorithm has to choose between ear-shaped, face shape, and whiskers. Which of these features results in the greatest purity of the labels on the left and right sub branches? Because it is if you can get to a highly pure subsets of examples, then you can either predict cat or predict not cat and get it mostly right."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### how pure is the set of examples?\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split2.png\" width=\"700\"/>\n",
    "\n",
    "The key question we need to answer is, given these three options of a feature to use at the root node, which one do we think works best? It turns out that rather than looking at these entropy numbers and comparing them, it would be useful to take a weighted average of them.\n",
    "\n",
    "If you apply the entropy formula from the last video to this left subset of data and this right subset of data, we find that the degree of impurity on the left is entropy of 0.8, which is about 0.72, and on the right, the entropy of 0.2 turns out also to be 0.72. This would be the entropy at the left and right subbranches if we were to split on the ear shape feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split3.png\" width=\"700\"/>\n",
    "\n",
    "In this example we have, five of the 10 examples went to the left sub-branch, so we can compute the weighted average as 5/10 times the entropy of 0.8, and then add to that 5/10 examples also went to the right sub-branch, plus 5/10 times the entropy of 0.2. Now, for this example in the middle, the left sub-branch had received seven out of 10 examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split4.png\" width=\"700\"/>\n",
    "\n",
    "The formula that we're actually going to use for choosing a split is not this way to entropy at the left and right sub-branches, instead is going to be the entropy at the root node, which is entropy of 0.5, then minus this formula. In this example, if you work out the math, it turns out to be 0.28. For the face shape example, we can compute entropy of the root node, entropy of 0.5 minus this, which turns out to be 0.03, and for whiskers, compute that, which turns out to be 0.12. These numbers that we just calculated, 0.28, 0.03, and 0.12, these are called the information gain, and what it measures is the reduction in entropy that you get in your tree resulting from making a split. Because the entropy was originally one at the root node and by making the split, you end up with a lower value of entropy and the difference between those two values is a reduction in entropy, and that's 0.28 in the case of splitting on the ear shape."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/information_gain.png\" width=\"700\"/>\n",
    "\n",
    "With this definition of entropy, and you can calculate the information gain associated with choosing any particular feature to split on in the node. Then out of all the possible futures, you could choose to split on, you can then pick the one that gives you the highest information gain. That will result in, hopefully, increasing the purity of your subsets of data that you get on the left and right sub-branches of your decision tree and that will result in choosing a feature to split on that increases the purity of your subsets of data in both the left and right sub-branches of your decision tree. Now that you know how to calculate information gain or reduction in entropy, you know how to pick a feature to split on another node."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build a large decision tree with multiple nodes\n",
    "\n",
    "The information gain criteria lets you decide how to choose one feature to split a one-node. Let's take that and use that in multiple places through a decision tree in order to figure out how to build a large decision tree with multiple nodes.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning_steps.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/recursive_splitting.png\" width=\"700\"/>\n",
    "\n",
    "Based on computing information gain for all three features, decide that ear-shaped is the best feature to split on. Based on that, we create a left and right sub-branches and send the subsets of the data with pointy versus floppy ear to left and right sub-branches.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/one_hot_encoding.png\" width=\"700\"/>\n",
    "<br>\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/one_hot_and_nn.png\" width=\"700\"/>\n",
    "\n",
    "With a **one-hot encoding** you can get your **decision tree** to work on features that can take on more than two discrete values and you can also apply this to **new network** or **linear regression** or **logistic regression** training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Continuous valued features\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/splitting_continuous_var.png\" width=\"700\"/>\n",
    "\n",
    "Try different thresholds, do the usual information gain calculation and split on the continuous value feature with the selected threshold if it gives you the best possible information gain out of all possible features to split on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_a_split.png\" width=\"700\"/>\n",
    "\n",
    "A good way to choose a split would be to just choose the value of the weighted variance that is lowest. Similar to when we're computing information gain, I'm going to make just one more modification to this equation. Just as for the classification problem, we didn't just measure the average weighted entropy, we measured the reduction in entropy and that was information gain."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tree ensembles\n",
    "\n",
    "One of the weaknesses of using a **single decision tree** is that that decision tree can be highly sensitive to small changes in the data. One solution to make the arrow less sensitive or more robust is to build not one decision tree, but to **build a lot of decision trees**, and we call that a **tree ensemble.**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/build_subtrees.png\" width=\"700\"/>\n",
    "\n",
    "the highest information gain feature to split on becomes the whiskers feature instead of the ear shape feature. As a result of that, the subsets of data you get in the left and right sub-trees become totally different and as you continue to run the decision tree learning algorithm recursively, you build out totally different sub trees on the left and right\n",
    "\n",
    "you get more accurate predictions if you train not just a single decision tree but a whole bunch of different decision trees.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/tree_ensemble.png\" width=\"700\"/>\n",
    "\n",
    "This is what we call a tree ensemble, which just means a collection of multiple trees, based on **different root**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/tree_ensemble_to_vote.png\" width=\"700\"/>\n",
    "\n",
    "the first tree would carry out inferences like this and predict that it is a cat. The second tree's inference would follow this path through the tree and therefore predict that is not cat. The third tree would follow this path and therefore predict that it is a cat. These three trees have made different predictions and so what we'll do is actually get them to vote.\n",
    "\n",
    "The reason we use an ensemble of trees is by having lots of decision trees and having them vote, it makes your overall algorithm less sensitive to what any single tree may be doing because it gets only one vote out of three or one vote out of many, many different votes and it makes your overall algorithm more robust."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sampling with replacement\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/sampling_with_replacement.png\" width=\"700\"/>\n",
    "\n",
    "The way we'll do so is we're reaching and pick out one random training example. Let's say we get this training example. Then we put it back into the bag, and then again randomly pick out one training example and so you get that. You pick again and again and again. Notice now this fifth training example is identical to the second one that we had out there. But that's fine. You keep going and keep going, and we get another repeats the example, and so on and so forth. Until eventually you end up with 10 training examples, some of which are repeats. You notice also that this training set does not contain all 10 of the original training examples, but that's okay. That is part of the sampling with replacement procedure. The process of sampling with replacement, lets you construct a new training set that's a little bit similar to, but also pretty different from your original training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random forest algorithm\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/random_forest_example3.png\" width=\"600\"/>\n",
    "\n",
    "Random forest, like its name implies, consists of a **large number of individual decision trees** that **operate as an ensemble**. Each individual tree in the random forest spits out a class prediction and the class with the **most votes** becomes our model’s prediction (see figure below).\n",
    "\n",
    "The fundamental concept behind random forest is a simple but powerful one — **the wisdom of crowds**. In data science speak, the reason that the random forest model works so well is:\n",
    "\n",
    "**A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.**\n",
    "\n",
    "**!!The low correlation between models is the key**\n",
    "\n",
    "**The reason for this wonderful effect is that the trees protect each other from their individual errors** While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Bagging (Bootstrap Aggregation) — Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly different tree structures. Random forest takes advantage of this by allowing each individual tree to **randomly sample from the dataset with replacement**, resulting in different trees. This process is known as bagging.\n",
    "\n",
    "**So in our random forest, we end up with trees that are not only trained on different sets of data (thanks to bagging) but also use different features to make decisions.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/random_forest.png\" width=\"700\"/>\n",
    "\n",
    "Random forest algorithm which is one powerful tree on sample algorithm that works much better than using a single decision tree.\n",
    "\n",
    "If you look carefully, you may notice that some of the training examples are repeated and that's okay. And if you train the decision on this data said you end up with this decision tree. And having done this once, we would then go and repeat this a second time. Use **something with replacement** to generate another training set of M or 10 training examples. This again looks a bit like the original training set but it's also a little bit different. You then **train the decision tree on this new data** set and you end up with a somewhat different decision tree. And so on. And you may do this a total of capital B times. Typical choice of capital B the number of such trees you built might be around a 100 people recommend any value from Say 64, 228. And having built an ensemble of say 100 different trees. ***you would then when you're trying to make a prediction, get these trees all votes on the correct final prediction.***\n",
    "\n",
    "This specific instance creation of tree ensemble is sometimes also called a bagged decision tree. And that's why also we use the let us lower case B an uppercase B here because that **stands for bag.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/random_forest_algo.png\" width=\"700\"/>\n",
    "\n",
    "One way to think about why this is more robust to than a single decision tree is the something with replacement procedure causes the algorithm to explore a lot of small changes to the data already and it's training different decision trees and is averaging over all of those changes to the data that the something with replacement procedure causes.\n",
    "\n",
    "And so this means that any little change further to the training set makes it less likely to have a huge impact on the overall output of the overall random forest algorithm. Because it's already explored and it's averaging over a lot of small changes to the training set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/random_forest_algo.png\" width=\"700\"/>\n",
    "\n",
    "Today by far the most commonly used way or implementation of decision tree ensembles or decision trees there's an album called XGBoost. It runs quickly, the open source implementations are easily used, has also been used very successfully to win many machine learning competitions as well as in many commercial applications."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bagging and Boosting\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/bagging_boosting.png\" width=\"700\"/>\n",
    "\n",
    "1. Bagging– It creates a different training subset from sample training data with replacement & the final output is based on **majority voting**. For example,  Random Forest.\n",
    "\n",
    "2. Boosting– It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example,  ADA BOOST, XG BOOST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/bagging_ensemble_method.png\" width=\"500\"/>\n",
    "\n",
    "**Bagging**\n",
    "Bagging, also known as Bootstrap Aggregation is the ensemble technique used by random forest. Bagging chooses a random sample from the data set. Hence each model is generated from the samples (Bootstrap Samples) provided by the Original **Data with replacement known as row sampling**. This step of row **sampling with replacement is called bootstrap**. Now each model is trained independently which generates results. The final output is **based on majority voting after combining the results of all models**. This step which involves combining all the results and generating output based on majority voting is known as aggregation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_vs_random_forest.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_trees_vs_nn.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}