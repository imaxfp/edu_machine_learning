{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Binary classification with categorical (discrete values)\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/binary_classification_categorical.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree.png\" width=\"700\"/>\n",
    "\n",
    "What is a decision tree? Here's an example of a model that you might get after training a decision tree learning algorithm on the data set that you just saw. The model that is output by the learning algorithm looks like a tree, and a picture like this is what computer scientists call a tree.\n",
    "\n",
    "Based on the value of the ear shape of this example we'll either go left or go right. The value of the ear-shape with this example is pointy, and so we'll go down the left branch of the tree, like so, and end up at this oval node over here. We then look at the face shape of this example, which turns out to be round, and so we will follow this arrow down over here. The algorithm will make a inference that it thinks this is a cat."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree2.png\" width=\"700\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning.png\" width=\"700\"/>\n",
    "\n",
    "\n",
    "The first step of decision tree learning is, we have to decide what feature to use at the root node.\n",
    "\n",
    "What that means is we will decide to look at all of our training examples, all tangent examples shown here, I split them according to the value of the ear shape feature.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning2.png\" width=\"700\"/>\n",
    "\n",
    "The second step is focusing just on the left part or sometimes called the left branch of the decision tree to decide what nodes to put over there. In particular, what feature that we want to split on or what feature do we want to use next.\n",
    "\n",
    "What we'll do now is take these five examples and split these five examples into two subsets based on their value of the face shape."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning3.png\" width=\"700\"/>\n",
    "\n",
    "Having done this on the left part to the left branch of this decision tree, we now repeat a similar process on the right part or the right branch of this decision tree. Focus attention on just these five examples, which contains one captain for dogs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decision Tree. Make Decision 1\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/decision_tree_learning4.png\" width=\"700\"/>\n",
    "\n",
    "how do you choose what features to use to split on at each node? At the root node, as well as on the left branch and the right branch of the decision tree, we had to decide if there were a few examples at that node comprising a mix of cats and dogs.\n",
    "\n",
    "The decision tree learning algorithm has to choose between ear-shaped, face shape, and whiskers. Which of these features results in the greatest purity of the labels on the left and right sub branches? Because it is if you can get to a highly pure subsets of examples, then you can either predict cat or predict not cat and get it mostly right."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### how pure is the set of examples?\n",
    "\n",
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split2.png\" width=\"700\"/>\n",
    "\n",
    "The key question we need to answer is, given these three options of a feature to use at the root node, which one do we think works best? It turns out that rather than looking at these entropy numbers and comparing them, it would be useful to take a weighted average of them.\n",
    "\n",
    "If you apply the entropy formula from the last video to this left subset of data and this right subset of data, we find that the degree of impurity on the left is entropy of 0.8, which is about 0.72, and on the right, the entropy of 0.2 turns out also to be 0.72. This would be the entropy at the left and right subbranches if we were to split on the ear shape feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split3.png\" width=\"700\"/>\n",
    "\n",
    "In this example we have, five of the 10 examples went to the left sub-branch, so we can compute the weighted average as 5/10 times the entropy of 0.8, and then add to that 5/10 examples also went to the right sub-branch, plus 5/10 times the entropy of 0.2. Now, for this example in the middle, the left sub-branch had received seven out of 10 examples."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/choosing_split4.png\" width=\"700\"/>\n",
    "\n",
    "The formula that we're actually going to use for choosing a split is not this way to entropy at the left and right sub-branches, instead is going to be the entropy at the root node, which is entropy of 0.5, then minus this formula. In this example, if you work out the math, it turns out to be 0.28. For the face shape example, we can compute entropy of the root node, entropy of 0.5 minus this, which turns out to be 0.03, and for whiskers, compute that, which turns out to be 0.12. These numbers that we just calculated, 0.28, 0.03, and 0.12, these are called the information gain, and what it measures is the reduction in entropy that you get in your tree resulting from making a split. Because the entropy was originally one at the root node and by making the split, you end up with a lower value of entropy and the difference between those two values is a reduction in entropy, and that's 0.28 in the case of splitting on the ear shape."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"../../../img/2_advanced_learning_algorithms/week4/information_gain.png\" width=\"700\"/>\n",
    "\n",
    "With this definition of entropy, and you can calculate the information gain associated with choosing any particular feature to split on in the node. Then out of all the possible futures, you could choose to split on, you can then pick the one that gives you the highest information gain. That will result in, hopefully, increasing the purity of your subsets of data that you get on the left and right sub-branches of your decision tree and that will result in choosing a feature to split on that increases the purity of your subsets of data in both the left and right sub-branches of your decision tree. Now that you know how to calculate information gain or reduction in entropy, you know how to pick a feature to split on another node."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}