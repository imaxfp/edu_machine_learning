{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Why do we need Neural Network?\n",
    "\n",
    "1. Solving complex non linear hypothesis\n",
    "2. When a linear classifier doesn't work, this is what we can usually turn to\n",
    "3. We have a lot of features \"more than two, cannot solve it by logistic regression\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple neuron representation\n",
    "\n",
    "<img src=\"../img/nn/single_neuron_representation.png\" width=\"600\"/>\n",
    "\n",
    "#### Simple network representation\n",
    "\n",
    "<img src=\"../img/nn/simple_neuron_network.png\" width=\"600\"/>\n",
    "\n",
    "#### Activation and parametrization\n",
    "\n",
    "In neural networks, we use the same logistic function as in classification. This is called a sigmoid (logistic) activation function. In this situation, our \"theta\" parameters are sometimes called \"weights\"\n",
    "\n",
    "$$\\frac{1}{1+e^{-Î¸}^{Tx}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"../img/nn/sigmoid_activation_function.png\" width=\"600\"/>\n",
    "\n",
    "At a very simple level, neurons are basically computational units that take inputs ***(dendrites)*** as electrical inputs called ***(spikes)*** that are channeled to outputs ***(axons)*** . Dendrites are like the input features - (x1....xn) and the output is the result of our ***hypothesis function***.\n",
    "In this model our x0 input node is sometimes called the \"bias unit.\" It is always equal to 1.\n",
    "In neural networks, we use the same logistic or sigmoid function 'g' as in classification. $$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Sometimes call it a ***sigmoid*** (logistic) as a ***activation*** function. In this situation, our \"theta\" parameters are sometimes called \"weights\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Forward propagation explained\n",
    "\n",
    "We can se each of (a) hidden layer consist of the same logic as ***'logistic regression'***\n",
    "<br>\n",
    "<img src=\"../img/nn/forward_propagation.png\" width=\"700\"/>\n",
    "<br>\n",
    "We can see formula of standard 'logistic model'. Again, this neural network is doing is just like ***logistic regression***, <span style=\"color: red\"> except that rather than using the original features - x1, x2, x3, is using these new features - a1, a2, a3.\n",
    "</span> They themselves are ***learned as functions*** of the input. Instead of being constrained to feed the features x1, x2, x3 to ***logistic regression.*** It gets to learn its own features, a1, a2, a3, to feed into the ***logistic regression*** and as you can imagine depending on what parameters it chooses for theta 1.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/nn/NN_features.png\" width=\"600\"/>\n",
    "<br>\n",
    "\n",
    "You can end up with a better hypotheses than if you were constrained to use the raw features x1, x2 or x3.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Representation II\n",
    "\n",
    "<img src=\"../img/nn/nn_basic_2.png\" width=\"800\"/>\n",
    "\n",
    "In this section we'll do a vectorized implementation of the above functions. We're going to define a new variable\n",
    "\n",
    "$$ Z^jk $$\n",
    "  that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/nn/formula_short_form.png\" width=\"200\"/>\n",
    "\n",
    "Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compute a simple 'non linear' hypothesis\n",
    "<img src=\"../img/nn/non_linear_classification.png\" width=\"500\"/>\n",
    "\n",
    "By ***sigmoid*** function or ***activation*** function ve can build simple 'AND' and 'OR' function logic\n",
    "<br>\n",
    "<img src=\"../img/nn/and_function_simple_example.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"../img/nn/or_function_simple_example.png\" width=\"500\"/>\n",
    "\n",
    "#### Compute complex 'non linear' hypothesis\n",
    "\n",
    "<img src=\"../img/nn/complex_example_hidden_layers.png\" width=\"500\"/>\n",
    "\n",
    "#### NN for 4 classifier \"4 logistic regression classifier into single NN\"\n",
    "\n",
    "<img src=\"../img/nn/4_logistic_reg_in_NN.png\" width=\"500\"/>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}