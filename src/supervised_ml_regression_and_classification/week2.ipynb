{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple linear regression\n",
    "\n",
    "### Multiple features\n",
    " In the original version of linear regression, you had a single feature x, the size of the house and you're able to predict y, the price of the house. The model was fwb of x equals wx plus b.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/lr_with_single_feature.png\" width=\"450\"/>\n",
    "<br>\n",
    "\n",
    " if you did not only have the size of the house as a feature with which to try to predict the price, but if you also knew the number of bedrooms, the number of floors and the age of the home in years, etc...\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/lr_multiple_features.png\" width=\"450\"/>\n",
    "<br>\n",
    "\n",
    "More features you have it seems like this would give you a lot more information with which to predict the price\n",
    "To introduce a little bit of new notation, we're going to use the variables X_1, X_2, X_3 and X_4.\n",
    "\n",
    "As a concrete example, X superscript in parentheses 2, will be a vector of the features for the second training example, so it will equal to this 1416, 3, 2 and 40. Sometimes this is called a row vector rather than a column vector.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/lr_multiple_features2.png\" width=\"450\"/>\n",
    "<br>\n",
    "\n",
    "Sometimes in order to emphasize that this X^2 is not a number but is actually a list of numbers that is a vector, we'll draw an arrow on top of that just to visually show that is a vector and over here as well.\n",
    "\n",
    "Now that we have multiple features, let's take a look at what a model would look like.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/lr_model_multiparams.png\" width=\"450\"/>\n",
    "<br>\n",
    "\n",
    "You can think of this 0.1 as saying that maybe for every additional square foot, the price will increase by 0.1 1,000 or by 100, because we're saying that for each square foot, the price increases by 0.1, times 1,000, which is 100. Maybe for each additional bathroom, the price increases by 4,000  and for each additional floor the price may increase by 10,000 and for each additional year of the house's age, the price may decrease by 2,000, because the parameter is negative 2.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/dot_product.png\" width=\"450\"/>\n",
    "<br>\n",
    "\n",
    "The dot products of two vectors of two lists of numbers W and X, is computed by checking the corresponding pairs of numbers, W_1 and X_1 multiplying that, W_2 X_2 multiplying that, W_3 X_3 multiplying that, all the way up to W_n and X_n multiplying that and then summing up all of these products.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/lr/lr_multiple_features2.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/lr/question_multiple_lr.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/vectorization_part1.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Vectorization makes your code shorter, so hopefully easier to write and easier for you or others to read, and it also makes it run much faster. But honest, this magic behind vectorization that makes this run so much faster.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/vectorization_part2.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "For loop:\n",
    "The for loop like this runs without vectorization. If j ranges from 0 to say 15, this piece of code performs operations one after another.\n",
    "\n",
    "<br>\n",
    "\n",
    "Vectorization:\n",
    "The computer hardware with vectorization. The computer can get all values of the vectors w and x, and in a single-step, it multiplies each pair of w and x with each other all at the same time in parallel.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/vectorization_part2.png\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Maybe the speed difference won't be huge if you have 16 features, but if you have thousands of features and perhaps very large training sets, this type of vectorized implementation will make a huge difference in the running time of your learning algorithm.\n",
    "\n",
    "<br>\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/vectorization_example3.png\" width=\"500\"/>\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Python, NumPy and Vectorization Example\n",
    "\n",
    "Let's explore some operations using vectors.\n",
    "<a name=\"toc_40015_3.4.1\"></a>\n",
    "### 1 Indexing\n",
    "Elements of vectors can be accessed via indexing and slicing. NumPy provides a very complete set of indexing and slicing capabilities. Reference [Slicing and Indexing](https://NumPy.org/doc/stable/reference/arrays.indexing.html) for more details.\n",
    "**Indexing** means referring to *an element* of an array by its position within the array.\n",
    "**Slicing** means getting a *subset* of elements from an array based on their indices.\n",
    "NumPy starts indexing at zero so the 3rd element of an vector $\\mathbf{a}$ is `a[2]`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "a[2].shape: () a[2]  = 2, Accessing an element returns a scalar\n",
      "a[-1] = 9\n",
      "The error message you'll see is:\n",
      "index 10 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # it is an unofficial standard to use np for numpy\n",
    "import time\n",
    "\n",
    "#vector indexing operations on 1-D vectors\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "#access an element\n",
    "print(f\"a[2].shape: {a[2].shape} a[2]  = {a[2]}, Accessing an element returns a scalar\")\n",
    "\n",
    "# access the last element, negative indexes count from the end\n",
    "print(f\"a[-1] = {a[-1]}\")\n",
    "\n",
    "#indexs must be within the range of the vector or they will produce and error\n",
    "try:\n",
    "    c = a[10]\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2 Slicing\n",
    "Slicing creates an array of indices using a set of three values (`start:stop:step`). A subset of values is also valid. Its use is best explained by example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a         = [0 1 2 3 4 5 6 7 8 9]\n",
      "a[2:7:1] =  [2 3 4 5 6]\n",
      "a[2:7:2] =  [2 4 6]\n",
      "a[3:]    =  [3 4 5 6 7 8 9]\n",
      "a[:3]    =  [0 1 2]\n",
      "a[:]     =  [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "#vector slicing operations\n",
    "a = np.arange(10)\n",
    "print(f\"a         = {a}\")\n",
    "\n",
    "#access 5 consecutive elements (start:stop:step)\n",
    "c = a[2:7:1];\n",
    "print(\"a[2:7:1] = \", c)\n",
    "\n",
    "# access 3 elements separated by two\n",
    "c = a[2:7:2];\n",
    "print(\"a[2:7:2] = \", c)\n",
    "\n",
    "# access all elements index 3 and above\n",
    "c = a[3:];\n",
    "print(\"a[3:]    = \", c)\n",
    "\n",
    "# access all elements below index 3\n",
    "c = a[:3];\n",
    "print(\"a[:3]    = \", c)\n",
    "\n",
    "# access all elements\n",
    "c = a[:];\n",
    "print(\"a[:]     = \", c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 Single vector operations\n",
    "There are a number of useful operations that involve operations on a single vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a             : [1 2 3 4]\n",
      "b = -a        : [-1 -2 -3 -4]\n",
      "b = np.sum(a) : 10\n",
      "b = np.mean(a): 2.5\n",
      "b = a**2      : [ 1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "print(f\"a             : {a}\")\n",
    "# negate elements of a\n",
    "b = -a\n",
    "print(f\"b = -a        : {b}\")\n",
    "\n",
    "# sum all elements of a, returns a scalar\n",
    "b = np.sum(a)\n",
    "print(f\"b = np.sum(a) : {b}\")\n",
    "\n",
    "b = np.mean(a)\n",
    "print(f\"b = np.mean(a): {b}\")\n",
    "\n",
    "b = a ** 2\n",
    "print(f\"b = a**2      : {b}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4 Vector Vector element-wise operations\n",
    "Most of the NumPy arithmetic, logical and comparison operations apply to vectors as well. These operators work on an element-by-element basis. For example\n",
    "$$ c_i = a_i + b_i $$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary operators work element wise: [0 0 6 8]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, -2, 3, 4])\n",
    "print(f\"Binary operators work element wise: {a + b}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Of course, for this to work correctly, the vectors must be of the same size:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error message you'll see is:\n",
      "operands could not be broadcast together with shapes (5,) (2,) \n"
     ]
    }
   ],
   "source": [
    "#try a mismatched vector operation\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(\"The error message you'll see is:\")\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 Scalar Vector operations\n",
    "Vectors can be 'scaled' by scalar values. A scalar value is just a number. The scalar multiplies all the elements of the vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = 5 * a : [ 5 10 15 20]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "\n",
    "# multiply a by a scalar\n",
    "b = 5 * a\n",
    "print(f\"b = 5 * a : {b}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6 Vector Vector dot product\n",
    "The dot product is a mainstay of Linear Algebra and NumPy. This is an operation used extensively in this course and should be well understood. The dot product is shown below.\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/dot_product.gif\" width=\"650\"/>\n",
    "<br>\n",
    "\n",
    "The dot product multiplies the values in two vectors element-wise and then sums the result. Vector dot product requires the dimensions of the two vectors to be the same.\n",
    "Let's implement our own version of the dot product below:\n",
    "\n",
    "**Using a for loop**, implement a function which returns the dot product of two vectors. The function to return given inputs $a$ and $b$:\n",
    "$$ x = \\sum_{i=0}^{n-1} a_i b_i $$\n",
    "Assume both `a` and `b` are the same shape."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dot(a, b) = 24\n"
     ]
    }
   ],
   "source": [
    "def my_dot(a, b):\n",
    "    \"\"\"\n",
    "   Compute the dot product of two vectors\n",
    "\n",
    "    Args:\n",
    "      a (ndarray (n,)):  input vector\n",
    "      b (ndarray (n,)):  input vector with same dimension as a\n",
    "\n",
    "    Returns:\n",
    "      x (scalar):\n",
    "    \"\"\"\n",
    "    x = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        x = x + a[i] * b[i]\n",
    "    return x\n",
    "\n",
    "\n",
    "# test 1-D\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "print(f\"my_dot(a, b) = {my_dot(a, b)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try the same operations using np.dot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1-D np.dot(a, b) = 24, np.dot(a, b).shape = () \n",
      "NumPy 1-D np.dot(b, a) = 24, np.dot(a, b).shape = () \n"
     ]
    }
   ],
   "source": [
    "# test 1-D\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "print(f\"NumPy 1-D np.dot(a, b) = {c}, np.dot(a, b).shape = {c.shape} \")\n",
    "c = np.dot(b, a)\n",
    "print(f\"NumPy 1-D np.dot(b, a) = {c}, np.dot(a, b).shape = {c.shape} \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7 The Need for Speed: Vector vs For loop\n",
    "We utilized the NumPy library because it improves speed memory efficiency. Let's demonstrate:\n",
    "So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b) =  2501072.5817\n",
      "Vectorized version duration: 11.5621 ms \n",
      "my_dot(a, b) =  2501072.5817\n",
      "loop version duration: 4502.3260 ms \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)  # very large arrays\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"np.dot(a, b) =  {c:.4f}\")\n",
    "print(f\"Vectorized version duration: {1000 * (toc - tic):.4f} ms \")\n",
    "\n",
    "tic = time.time()  # capture start time\n",
    "c = my_dot(a, b)\n",
    "toc = time.time()  # capture end time\n",
    "\n",
    "print(f\"my_dot(a, b) =  {c:.4f}\")\n",
    "print(f\"loop version duration: {1000 * (toc - tic):.4f} ms \")\n",
    "\n",
    "del (a);\n",
    "del (b)  #remove these big arrays from memory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient descent for multiple linear regression\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/gradient_descent_for_multiple_linear.png\" width=\"650\"/>\n",
    "<br>\n",
    "\n",
    "Here's what gradient descent looks like. We're going to repeatedly update each parameter w_j to be w_j minus Alpha times the derivative of the cost J, where J has parameters w_1 through w_n and b. Once again, we just write this as J of vector w and number b. Let's see what this looks like when you implement gradient descent and in particular, let's take a look at the derivative term.\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/gradient_descent_for_multiple_linear.png\" width=\"650\"/>\n",
    "<br>\n",
    "\n",
    "The formula for the derivative of J with respect to w_1 on the right looks very similar to the case of one feature on the left. The error term still takes a prediction f of x minus the target y. One difference is that w and x are now vectors and just as w on the left has now become w_1 here on the right\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/gradient_descent_for_multiple_linear2.png\" width=\"650\"/>\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Notation\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/notation.png\" width=\"900\"/>\n",
    "<br>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Matrix X containing our examples\n",
    "Similar to the table above, examples are stored in a NumPy matrix `X_train`. Each row of the matrix represents one example. When you have $m$ training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$ is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "\n",
    "$$\\mathbf{X} =\n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\\n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element.\n",
    "\n",
    "Display the input data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (3, 4), X Type:<class 'numpy.ndarray'>)\n",
      "[[2104    5    1   45]\n",
      " [1416    3    2   40]\n",
      " [ 852    2    1   35]]\n",
      "y Shape: (3,), y Type:<class 'numpy.ndarray'>)\n",
      "[460 232 178]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "\n",
    "# data is stored in numpy array/matrix\n",
    "print(f\"X Shape: {X_train.shape}, X Type:{type(X_train)})\")\n",
    "print(X_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parameter vector w, b\n",
    "\n",
    "* $\\mathbf{w}$ is a vector with $n$ elements.\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "  - in our dataset, n is 4.\n",
    "  - notionally, we draw this as a column vector\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "* $b$ is a scalar parameter.\n",
    "\n",
    "For demonstration, $\\mathbf{w}$ and $b$ will be loaded with some initial selected values that are near the optimal. $\\mathbf{w}$ is a 1-D NumPy vector."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init shape: (4,), b_init type: <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init shape: {w_init.shape}, b_init type: {type(b_init)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Prediction With Multiple Variables\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
    "where $\\cdot$ is a vector `dot product`\n",
    "\n",
    "To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_vec shape (4,), x_vec value: [2104    5    1   45]\n",
      "f_wb shape (), prediction: 459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    single predict using linear regression\n",
    "    Args:\n",
    "      x (ndarray): Shape (n,) example with multiple features\n",
    "      w (ndarray): Shape (n,) model parameters\n",
    "      b (scalar):             model parameter\n",
    "\n",
    "    Returns:\n",
    "      p (scalar):  prediction\n",
    "    \"\"\"\n",
    "    p = np.dot(x, w) + b\n",
    "    return p\n",
    "\n",
    "\n",
    "# get a row from our training data\n",
    "x_vec = X_train[0, :]\n",
    "print(f\"x_vec shape {x_vec.shape}, x_vec value: {x_vec}\")\n",
    "\n",
    "# make a prediction\n",
    "f_wb = predict(x_vec, w_init, b_init)\n",
    "print(f\"f_wb shape {f_wb.shape}, prediction: {f_wb}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Cost With Multiple Variables\n",
    "The equation for the cost function with multiple variables $J(\\mathbf{w},b)$ is:\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2 \\tag{3}$$\n",
    "where:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b  \\tag{4} $$\n",
    "\n",
    "\n",
    "In contrast to previous labs, $\\mathbf{w}$ and $\\mathbf{x}^{(i)}$ are vectors rather than scalars supporting multiple features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 1.5578904428966628e-12\n"
     ]
    }
   ],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b  #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i]) ** 2  #scalar\n",
    "    cost = cost / (2 * m)  #scalar\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Compute and display cost using our pre-chosen optimal parameters.\n",
    "cost = compute_cost(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Descent With Multiple Variables\n",
    "Gradient descent for multiple variables:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
    "& w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{5}  \\; & \\text{for j = 0..n-1}\\newline\n",
    "&b\\ \\ = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "where, n is the number of features, parameters $w_j$,  $b$, are updated simultaneously and where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{6}  \\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{7}\n",
    "\\end{align}\n",
    "$$\n",
    "* m is the number of training examples in the data set\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Compute Gradient with Multiple Variables\n",
    "An implementation for calculating the equations (6) and (7) is below. There are many ways to implement this. In this version, there is an\n",
    "- outer loop over all m examples.\n",
    "    - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ for the example can be computed directly and accumulated\n",
    "    - in a second loop over all n features:\n",
    "        - $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$ is computed for each $w_j$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db at initial w,b: -1.6739251501955248e-06\n",
      "dj_dw at initial w,b: \n",
      " [-2.73e-03 -6.27e-06 -2.22e-06 -6.92e-05]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m, n = X.shape  #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw\n",
    "\n",
    "\n",
    "#Compute and display gradient\n",
    "tmp_dj_db, tmp_dj_dw = compute_gradient(X_train, y_train, w_init, b_init)\n",
    "print(f'dj_db at initial w,b: {tmp_dj_db}')\n",
    "print(f'dj_dw at initial w,b: \\n {tmp_dj_dw}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Descent With Multiple Variables\n",
    "The routine below implements equation (5) above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost  2529.46   \n",
      "Iteration  100: Cost   695.99   \n",
      "Iteration  200: Cost   694.92   \n",
      "Iteration  300: Cost   693.86   \n",
      "Iteration  400: Cost   692.81   \n",
      "Iteration  500: Cost   691.77   \n",
      "Iteration  600: Cost   690.73   \n",
      "Iteration  700: Cost   689.71   \n",
      "Iteration  800: Cost   688.70   \n",
      "Iteration  900: Cost   687.69   \n",
      "b,w found by gradient descent: -0.00,[ 0.2   0.   -0.01 -0.07] \n",
      "prediction: 426.19, target value: 460\n",
      "prediction: 286.17, target value: 232\n",
      "prediction: 171.47, target value: 178\n"
     ]
    }
   ],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "      \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)  ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw  ##None\n",
    "        b = b - alpha * dj_db  ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i < 100000:  # prevent resource exhaustion\n",
    "            J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b, J_history  #return final w,b and J history for graphing\n",
    "\n",
    "\n",
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init)\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,\n",
    "                                            compute_cost, compute_gradient,\n",
    "                                            alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m, _ = X_train.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot cost versus iteration\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(12, 4))\n",
    "ax1.plot(J_hist)\n",
    "ax2.plot(100 + np.arange(len(J_hist[100:])), J_hist[100:])\n",
    "ax1.set_title(\"Cost vs. iteration\")\n",
    "ax2.set_title(\"Cost vs. iteration (tail)\")\n",
    "ax1.set_ylabel('Cost')\n",
    "ax2.set_ylabel('Cost')\n",
    "ax1.set_xlabel('iteration step')\n",
    "ax2.set_xlabel('iteration step')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Practice quiz: Multiple linear regression\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/multiple_lr/questions.png\" width=\"700\"/>\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature scaling\n",
    "\n",
    "Technique called feature scaling that will enable gradient descent to run much faster.\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/features_scaling_params.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/features_scaling_and_gradient.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "if you run gradient descent on a cost function to find on this, re scaled x1 and x2 using this transformed data, then the contours will look more like this more like circles and less tall and skinny. And **gradient descent can find a much more direct path to the global minimum.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can implement feature scaling, to take features that take on very different ranges of values and skill them to have comparable ranges of values to each other:\n",
    "<br>\n",
    "One way to get a scale version of x_1 is to take each original x1_ value and divide by 2,000, the maximum of the range. The scale x_1 will range from 0.15 up to one. Similarly, since x_2 ranges from 0-5, you can calculate a scale version of x_2 by taking each original x_2 and dividing by five, which is again the maximum.\n",
    "\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_example.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "By Mean Normalization, you may find that the average of feature 1, Mu_1 is 600 square feet. Let's take each x_1, subtract the mean Mu_1, and then let's divide by the difference 2,000 minus 300, where 2,000 is the maximum and 300 the minimum, and if you do this, you get the normalized x_1 to range from negative 0.18-0.82. Similarly, to mean normalized x_2, you can calculate the average of feature 2. For instance, Mu_2 may be 2.3. Then you can take each x_2, subtract Mu_2 and divide by 5 minus 0. Again, the max 5 minus the mean, which is 0. The mean normalized x_2 now ranges from negative 0.46-0 54. If you plot the training data using the mean normalized x_1 and x_2, it might look like this.\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_mean_normalization.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "To implement Z-score normalization, you need to calculate something called the standard deviation of each feature. sometimes also called the Gaussian distribution, this is what the standard deviation for the normal distribution looks like.\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_zi_score.png\" width=\"700\"/>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_when.png\" width=\"700\"/>\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Question\n",
    "\n",
    "Which of the following is a valid step used during feature scaling?\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_question_1.png\" width=\"400\"/>\n",
    "<br>\n",
    "\n",
    "<img src=\"../../img/coursera/supervised/feature_scaling_question_1_answer.png\" width=\"700\"/>\n",
    "<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Choosing the learning rate\n",
    "\n",
    "Concretely, if you plot the cost for a number of iterations and notice that the costs sometimes goes up and sometimes goes down, you should take that as a clear sign that gradient descent is ***not working properly.***\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}