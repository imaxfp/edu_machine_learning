{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ML system design\n",
    "\n",
    "### System Design Example: supervised learning example spam/not spam\n",
    "\n",
    "Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set.  If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not.\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/system_design/spam_ham_example.png\" width=\"600\"/>\n",
    "\n",
    "So how could you spend your time to improve the accuracy of this classifier?\n",
    "1. Collect lots of data (for example \"honeypot\" project but doesn't always work)\n",
    "2. Develop sophisticated features (for example: using email header data in spam emails)\n",
    "3. Develop algorithms to process your input in different ways (recognizing misspellings in spam)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Error Analysis\n",
    "\n",
    "The recommended approach to solving machine learning problems is to:\n",
    "- Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.\n",
    "- Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "- Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made.\n",
    "\n",
    "<br>\n",
    "<img src=\"../img/system_design/nlp_stemming.png\" width=\"600\"/>\n",
    "\n",
    "### Error metrics for Skewed Classes\n",
    "\n",
    "<img src=\"../img/error_metrics/precision_recall.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"../img/error_metrics/trading_off_precision_recall.png\" width=\"600\"/>\n",
    "<br>\n",
    "<img src=\"../img/error_metrics/f1_score.png\" width=\"600\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data For Machine Learning\n",
    "\n",
    "#### It's not who has the best algo that wins. It's who has the most data.\n",
    "\n",
    "<img src=\"../img/system_design/the_most_data.png\" width=\"600\"/>\n",
    "\n",
    "### QUESTIONS:\n",
    "\n",
    "Having a large training set can help significantly improve a learning algorithm’s performance. However, the large training set is unlikely to help when:\n",
    "\n",
    "- The features x do not contain enough information to predict y accurately (such as predicting a house’s price from only its size), and we are using a simple learning algorithm such as logistic regression.\n",
    "- The features x do not contain enough information to predict y accurately (such as predicting a house’s price from only its size), even if we are using a neural network with a large number of hidden units.\n",
    "\n",
    "\n",
    "1.\n",
    "<img src=\"../img/system_design/recall_calculation.png\" width=\"900\"/>\n",
    "\n",
    "2. Suppose a massive dataset is available for training a learning algorithm. Training on a lot of data is likely to give good performance when two of the following conditions hold true.\n",
    "\n",
    "- We train a learning algorithm with a large number of parameters (that is able to learn/represent fairly complex functions).\n",
    "    (You should use a \"low bias\" algorithm with many parameters, as it will be able to make use of the large dataset provided. If the model has too few parameters, it will underfit the large training set.)\n",
    "\n",
    "- The features 'x' contain sufficient information to predict 'y' accurately.  (For example, one way to verify this is if a human expert on the domain can confidently predict y when given only x).\n",
    "    (It is important that the features contain sufficient information, as otherwise no amount of data can solve a learning problem in which the features do not contain enough information to make an accurate prediction.)\n",
    "\n",
    "3. The classifier is likely to now have higher recall. Suppose you have trained a logistic regression classifier which is outputing hθ(x) Currently, you predict 1 if hθ(x)≥threshold and predict 0 if hθ(x)<threshold where currently the threshold is set to 0.5.\n",
    "\n",
    "- The classifier is likely to now have higher recall.\n",
    "    (Lowering the threshold means more y = 1 predictions. This will increase the number of true positives and decrease the number of false negatives, so recall will increase.)\n",
    "\n",
    "4. Suppose you are working on a spam classifier, where spam emails are positive examples (y=1) and non-spam emails are negative examples (y=0). You have a training set of emails in which 99% of the emails are non-spam and the other 1% is spam. Which of the following statements are true?\n",
    "\n",
    "- If you always predict non-spam (output y=0), your classifier will have an accuracy of 99%.\n",
    "    (Since 99% of the examples are y = 0, always predicting 0 gives an accuracy of 99%. Note, however, that this is not a good spam system, as you will never catch any spam.)\n",
    "\n",
    "- A good classifier should have both a high precision and high recall on the cross validation set.\n",
    "    (For data with skewed classes like these spam data, we want to achieve a high F1 score, which requires high precision and high recall)\n",
    "\n",
    "- If you always predict non-spam (output y=0), your classifier will have 99% accuracy on the training set, and it will likely perform similarly on the cross validation set.\n",
    "    (The classifier achieves 99% accuracy on the training set because of how skewed the classes are. We can expect that the cross-validation set will be skewed in the same fashion, so the classifier will have approximately the same accuracy.)\n",
    "\n",
    "\n",
    "5.\n",
    "- On skewed datasets (e.g., when there are more positive examples than negative examples), accuracy is not a good measure of performance and you should instead use F1 score based on the precision and recall.\n",
    "    (You can always achieve high accuracy on skewed datasets by predicting the most the same output (the most common one) for every input. Thus the F1 score is a better way to measure performance.)\n",
    "\n",
    "- Using a very large training set makes it unlikely for model to overfit the training data.\n",
    "    (A sufficiently large training set will not be overfit, as the model cannot overfit some of the examples without doing poorly on the others.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}